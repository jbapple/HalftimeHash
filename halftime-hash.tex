\documentclass[sigconf, nonacm]{acmart}

%% \documentclass[acmsmall, nonacm]{acmart}

%% \geometry{twoside=false,
%%   includeheadfoot, head=13pt, foot=2pc,
%%   paperwidth=6in, paperheight=8in,
%%   top=58pt, bottom=44pt, inner=46pt, outer=46pt,
%%   marginparwidth=2pc,heightrounded
%% }%


%\usepackage[margin=1in]{geometry}
%\usepackage{accsupp}
%\usepackage{moreverb}
\usepackage{amsfonts}
\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{color}
%\usepackage{comment}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{microtype}
\usepackage{multirow}
%\usepackage{newpxtext}
%\usepackage{verbatim}
%\usepackage{cleveref}


%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{http://vldb.org/pvldb/format_vol14.html}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 

\lstloadlanguages{C++}

\lstset{%
  basicstyle=\ttfamily,%\small,
  language=C++,
%  breaklines=true,
  columns=fullflexible,
%  identifierstyle=\color{red},
  showstringspaces=false,
  commentstyle=\color[rgb]{0.5,0,0}\itshape,
  %backgroundcolor=\color[rgb]{0.92,0.92,1}
}

%\pagenumbering{gobble}
%\pagestyle{empty}

\newtheorem{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{claim}{Claim}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\rats}{\mathbb{Q}}
\newcommand{\nats}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\cplx}{\mathbb{C}}

%% \lstset{basicstyle=\ttfamily,
%% escapeinside={||},
%% mathescape=true}

\newenvironment{blockquote}
{\begin{quote}\itshape}
{\end{quote}}
\title{HalftimeHash: modern hashing without 64-bit multipliers, prime numbers, or finite fields}
\author{Jim Apple}
\orcid{0000-0002-8685-9451}
\email{jbapple@apache.org}

\begin{document}
%\thispagestyle{fancyplain}
%\thispagestyle{empty}

\begin{abstract}
HalftimeHash is a new algorithm for hashing long strings.
The goals are low ``collisions'' (different inputs that produce identical output hash values), high throughput, and {\em longevity} -- HalftimeHash should be easy to implement correctly on new ISAs and easy to code in almost any programming language.
\end{abstract}

\maketitle

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
\endgroup
}
%%% VLDB block end %%%

\section{Introduction}
A hash family is a map from a set of seeds, $S$ and a domain $D$ to a codomain $C$.\footnote{Seeds are sometimes called {\em salts} or {\em keys}. Given that elements of the domain of hash functions are also sometimes called {\em keys}, this work will avoid this term altogether, for the sake of clarity.}
A hash family $H$ is called is $\varepsilon$-almost universal (``$\varepsilon$-AU'' or just AU\footnote{Technically, all hash families are 1-AU. In this work, a function will be called AU iff it is $o(1)$-AU.}) when
\[\forall x,y \in D, x \neq y \implies \mathrm{Pr}_{s \in S}[H(s, x) = H(s, y)] \leq \varepsilon\]
The intuition behind this definition is that the input to be hashed may night be random, but collisions can still be made unlikely and random by picking randomly from a hash {\em family}, rather than anchoring on a specific hash {\em function}.

HalftimeHash is a new hash family designed for

\begin{enumerate}
\item High speed on long strings.
  This should enable hybrid hash families that hash long strings with HalftimeHash and short strings with a hash family that is designed with short strings in mind. \cite{siphash,umash}
\item Simple implementation that is easily portable between programming languages and machine ISA's.
\item Tunable output length for applications that require it, such as universally unique identifiers.
\end{enumerate}

For HalftimeHash, the family parameters are:

\begin{description}
\item[Domain $D$] consists of all strings of length less than $2^{64}$ bytes.
\item[Codomain $C$] is 16, 24, or 32 bytes.
  These are easily expanded to larger sizes.
\item[Collision probability $\varepsilon$] is less than than $2^{-83}$ bits for input shorter than one exabytes (for the family producing 24 bytes of output), with longer strings producing lower collision possibility.
\item[Seeds $S$] are larger for longer strings in the domain, but are less than 29KB for strings as long as a exabyte.
\end{description}

HalftimeHash is a ``universe collapsing'' function, designed to hash long strings into short ones. \cite{linear-hash-functions,hashing-without-primes-revisited,cuckoo-journal}
This differs from short-input families like SipHash, suitable for hashing short strings down to 64 bits, or general purpose families like UMASH, suitable for hashing strings of any length down to 64 bits.
Universe collapsing functions are especially suitable for pairing with short-input families: when $n$ items are to be handled by a hash-based algorithm, even if they are very long, a universe-collapsing function that reduces them to hash values of length $c \lg n$ bits for some suitable $c \geq 3$ produces no collisions with high probability.
Short-input families can then be used on those $c \lg n$ bits. \cite{universe-collapse-linear-probing,siphash,tabulation,simple-hash-functions-work}

\subsection{Prior work}

Hash family invention is a significant hobbyist community, with a number of fast hash algorithms running at rates exceeding 8 bytes per cycle on modern Intel processors, including Fast Positive Hash, falk\-hash, xxh, Meow\-Hash, and UMASH, and cl\-hash. \cite{smhasher}
Of these, only cl\-hash and U\-MASH include claims of being AU.

Other previous work on AU hashing long strings is no longer as competitive, performance-wise, as it once was, including UHASH and VHASH.
UHASH was designed specifically for SIMD speed, but it predates Nandi's breakthrough EHC algorithm and hard-codes the vector width. \cite{umac}
VHASH similarly came about before EHC and is an update to UHASH designed to take advantage of 64-bit architectures, and performs much better than UHASH, but still lags compared to the current fastest hash families. \cite{vmac,smhasher}

\subsection{Results}

HalftimeHash uses less than 1000 lines of code in C++, does not use any assembly, and makes spare use of the C++ standard library.
It can take advantage of vector ISA extensions like AVX-512, AVX2, and SSE, but it also runs on the C++ virtual machine model with no architecture-specific features.
It is a parameterized algorithm that supports updates when vector sizes increase or longer hash values are needed.
On strings longer than 1KB, some variants are up to 130\% faster than CLHASH, the AU hash family that comes closest in performance to HalftimeHash.

\section{Conventions}

$H$ and $G$ will denote hash families.
Generally, capital letters will denote sets or functions, including linear transformations.
Linear transformations and their matrices are discussed interchangeably; any vector space basis will do.

Lowercase letters will usually represent strings or non-negative integers.
$n$ generally denotes the size of the input string, usually in 64-bit words.
Strings are sometimes instead treated as sequences of bytes or of 32-bit words.
Arithmetic operations may be over elements of $\ints_m$, $\ints$, or sometimes $\rats$.

Subscripts used to indicate a numbered component of a sequence, usually numbered from 0.

$\varepsilon$ is called the {\em collision probability} of $H$; it is inversely related to $H$'s {\em entropy}, $-\lg \varepsilon$.
By convention, the entropy of a hash family with collision probability of $0$ is $+\infty$.
(An example of a hash family like this is the identity function variant $(s, x) \mapsto x$.)

HalftimeHash takes two arguments, the {\em seed} and the {\em input}.
The seed is sometimes referred as {\em input entropy}, which is distinguished from $-\lg \varepsilon$ both because it is input and because it is measured in words or bytes, not as an implicit aspect of the output and measured in bits.

Each step of HalftimeHash applies various transforms to groups of input values.
These groups are called {\em instances}.
Instances are logically contiguous but physically strided, for the purpose of making SIMD vector processing faster and simpler.
The number of 64-bit words between two 64-bit words in the same instance, plus one (for a fencepost), is the expected SIMD parallelism.
This physically contiguous set is called a {\em block}; the number of 64-bit words in a block is called the {\em block size}.
The processing of a transform on a single instance is called an {\em execution}.
Because instances are logically contiguous, when possible, the text will elide references to the block size.

``32-bit multiplication'' will mean multiplying two unsigned 32-bit words and producing a single 64-bit word.
``64-bit multiplication'' similarly refers to the operation producing a 128-bit product.
All machine integers are unsigned.

HalftineHash produces output that is collision resistant among strings of the same length.
Adding collision resistance between strings of different lengths to such a hash family requires only appending the length at the end of the output.
This turns, for instance, a hash family that produces 24 bytes of output from a string of length less than $2^64$ into a hash family that produces 32 bytes of output.

The main portion of the text describes a particular instantiation of HalftimeHash that produces 24 bytes of output.
This will be generalized in the analysis section.
Variants will be specified by their number of output bits: HalftimeHash16, HalftimeHash24, HalftimeHash32, or HalftimeHash40.

Except where otherwise mentioned, all benchmarks were run on an Intel i7-7800x, running Ubuntu 18.04, with g++ 10.1.0-2ubuntu1~18.04.

\section{Algorithm}
\label{algo}

\subsection{Overview}

The macrostructure of HalftimeHash is in the style of tree hash, as described by Carter and Wegman. \cite{carter-wegman-79}
The leaves of a tree are the characters of the input string, while the root is the output hash value.
Each level of edges is represented by a single hash function chosen at random from an AU family.

Algebraically, a tree hash family consists of an alphabet $\Sigma$ and a set of almost-universal hash families $H_1, H_2, \dots$.
The domain of $H_1$ is $\Sigma^k$ for some $k$.
If the codomain of $H_i$ is $C$, the domain of $H_{i+1}$ is $C^k$ for some $k$.
Each execution of a tree hash family is defined recursively.
An execution of height $1$ is a pair consisting of an input and output value for $H_1$.
An execution of height $n+1$ is $m$ applications of level $n$, where $m$ is the dimension of the domain of $H_{n+1}$ over the codomain of $H_n$, along with the output value that is the result of applying $H_{n+1}$ to the output values of the $k$ applications of level $n$.

More concretely, $H1(a,b)$ is a tree hash execution of height 1, $H2(H1(a,b), H1(c,d)$ is a tree hash execution of height 2, and so on.

Carter and Wegman show that if each $H_i$ is $\varepsilon_i$-AU, then an application of height $n$ is $\sum_{i=1}^n \varepsilon_i$-AU.
%The Badger construction (\cite{badger}) shows how to manage strings where there is a mismatch in the input and output arity at some levels without negatively impacting the universality.

A novel implementation choice in HalftimeHash is using an erasure coding of the input in the leaves.
The central idea is called ``EHC'', for {\em Encode, Hash, Combine}. \cite{ehc-nandi}
The application of the erasure code adds ``minimum distance'' $d$, so that any two input values that differ in any location produce encoded outputs that differ in $d > 1$ locations.
This decreases the probability of collisions by a factor of $\varepsilon^{d-1}$.

\includegraphics[width=8cm]{Diagram2.eps}
\captionof{figure}{An example of the tree hash shape used in HalftimeHash.
  At the leaves are the input characters to be hashed.
  Each block of seven characters is initially encoded and then hashed down to three characters in a process called ``EHC''. \cite{ehc-nandi}
  Following that, three copies of Carter-Wegman-style tree hash run on each of the components produced during the EHC step.
}

This section focuses on a particular instantiation of this formula with distance $3$, producing a hash value $24$ bytes in size.
Adding variants with different distances is direct, and the benchmarking results below include variants with distance $2$ and distance $4$.

TODO: VTune on l3 latency and RAM bandwidth limitations

 TODO: Compare with UHASH, which is also 32-bit

\subsection{The NH hashing primitive}

The hashing primitive used in HalftimeHash to hash small, fixed-length sequences is called {\em NH}. \cite{umac}
NH's domain is the same as its set of seeds: sequences with an even number of 32-bit unsigned integers.
The codomain is 64 bit integers.
The definition of NH is:

$$\sum_{i=0}^m (d_{2i} + s_{2i})(d_{2i+1} + s_{2i+1})$$

The additions are in the ring $\ints_{2^{32}}$, while the multiplication is in the ring $\ints_{2^{64}}$.
Unusually, NH has only $32$ bits of entropy; typically universal families with $64$ bits of output have nearly $64$ bits of entropy. \cite{umash,clhash}
With NH, typically one of a number of other algorithms is used in a final additional step to reduce the size of the output to $32$ bits.
Creating additional output entropy bits is usually done by running the algorithm twice. \cite{umash,umac}
Despite this potential extra hashing (depending on needed entropy), NH has been wildly successful.
It is used in UMAC \cite{umac}, VMAC \cite{vmac}, CLHASH \cite{clhash}, UMASH \cite{umash}, XXH3 \cite{xxh3}, and more, usually with $k = 64$.

In HalftimeHash, no 64-bit-mul\-ti\-pli\-ca\-tions are needed - no 64-bit values are multiplied with each other.
This is in support of two goals --
the first is portability to platforms or programming languages without native unsigned 64-bit-by-64bit-yielding-128-bit multiplications.
Languages like Java, Python, and Haskell can do these long multiplications, but not without slipping into arbitrary-precision-integer code, decreasing performance substantially.
Many programming languages do not include a way to perform native carryless (aka polynomial) multiplication, which is the central operation in the formerly fastest AU hash functions. \cite{umash,clhash}

TODO: check this

TODO: what about JavaScript

The other is SIMD-friendliness.
The x86-64 ISA extensions SSE2, AVX2, and AVX512F all contain instruction to simultaneously multiply multiple pairs of 32-bit words, producing 64-bit values.
Aarch64 has similar instructions in the NEON set, and, unlike x86-64 chips, it cannot produce a single 128-bit product out of two 64-bit multiplicands with one instruction -- each half of the product requires its own instruction.
% The POWER ISA also contains this, but only on 128 bits at once?

%The code above is the key code needed to operate on 512-bit vectors in AVX-512. Similar code needs to be present for chips with more limited ISAs.

%We also include code that should work on all implementations of C++11, even those with no SIMD.

Another appealing aspect of the NH primitive is that it is not only almost-universal, it is also {\em$ \varepsilon$-almost $\Delta$-universal} ($\varepsilon$-A$\Delta$U):
\[\forall \delta \in C, \forall x,y \in D, x \neq y \implies \mathrm{Pr}_{s \in S}[H(s, x) - H(s, y) = \delta] \leq \varepsilon \]
Almost-$\Delta$ universality reduces the number of multiplications required.
Given an almost-$\Delta$ universal family $H$ from $D$ to $C$, define a hash family $H'$ from $D \times C$ to $C$ as $H'_s ((d, x)) \triangleq H_s(d) + x$, for $x \in C$; $H'$ is AU, though not A$\Delta$U.
This saves a multiplication over a construction like $H_s(d) + G_{s'}(x)$ (for any almost-universal family $G$ from $C$ to $C$).
In HalftimeHash, the multiplication savings tradeoff with increased entropy requirements via a parameter the caller can set called ``fanout''
%The default is 8, saving $12.5\%$ of the erstwhile $n/8 \pm O(1)$ multiplications to hash strings of length $n$.

%Now we can build the hash function referenced above, which takes a Block of values and a Block of entropy, spitting out a hashed Atom that can later be collapse via %addition to yield the almost-delta-universal NH family.

\subsection{Encode-Hash-Combine}

At the leaves of the tree, HalftimeHash uses an algorithm from Nandi called ``EHC'' for {\em encode-hash-combine}: \cite{ehc-nandi}

\begin{enumerate}
\item A sequence of items is encoded with an erasure code with {\em minimum distance $d$}.
  In this context, ``distance'' means the Hamming distance -- the number of unequal items between two sequences.
  An encoding with minimum distance $d$ means that all sequences that are unequal in {\em any} place before encoding are unequal in at least $d$ places after encoding.
  For example, encoding $(a,b)$ as $(a,b,a+b)$ has minimum distance 2.
\item Each item in the encoded sequence is hashed, with an independently chosen seed, to a field.
  HalftimeHash will use NH here (the AU variant, not the A$\Delta$U one)
\item The resulting sequence of hash values is put through a linear transformation on that field, $T$.
  $T$ must have the property that any $d$ columns of it are independent, and so a square matrix made up of any $d$ columns is injective.
%  A linear transformation with this property is said to be {\em d-MDS}.
\end{enumerate}

Nandi proves that if the hash family used in step 2 is $\varepsilon$-A$\Delta$U, then the output of step 3 is $\varepsilon^d$-A$\Delta$U.
To avoid expensive operations over finite fields, HalftimeHash uses a non-linear erasure code and a linear transformation $T$ with a looser constraint on the invertability; see below.

Additionally, the other components of HalftimeHash do not depend on EHC being A$\Delta$U, only AU.
Just as in NH, this saves HalftimeHash a percentage of multiplications.
See Section \ref{analysis} for more details.

\subsubsection{Erasure coding}

One key aspect of the efficiency of HalftimeHash is that the additional size added by encoding the input string is low as a percentage of the total hashed data.
%By the Singleton bound, A code with minimum distance $d$ must be at least $d-1$ characters longer than the pre-encoded data.
In order to use large blocks, the dimension of the vector space that is the domain of $T$ must grow as well.
For finite fields, this is not an issue, but in $\ints_{2^{64}}$, as discussed below, it becomes a constraint satisfaction problem to find matrices that preserve entropy.
%While this work can be done offline, it's not obvious the effort will yield good results.

Instead of using large blocks, HalftimeHash uses a short erasure code but uses SIMD to maintain high throughput.
All operations extend pointwise, and HalftimeHash hashes eight different parts of the string simultaneously.\footnote{Fewer than eight are interleaved for narrower SIMD registers like AVX2's 256-bit registers.}

HalftimeHash uses a non-linear erasure code discovered by Gab\-ri\-el\-yan. \cite{9-7-erasure-code}
It maps seven 3-tuples to nine 3-tuples with a minimum distance of three.
Once the encoding is done, the hash function in step 2 doesn't just add randomness, it also reduces the size of the universe, by hashing each 3-tuple to a single 64-bit value.

Gabrielyan's code produces two additional 3-tuples without altering the seven inputs.\footnote{This is called a ``systematic'' code.}
The first additional tuple is just the \texttt{xor} of the seven input words.
The second tuple is the following sum, numbering the input words as $(x_0, y_0, z_0), (x_1, y_1, z_1), \dots$.

\begin{displaymath}
  \begin{array}{rlcr}
       & (x_0, & y_0, & z_0)\\
\oplus & (y_1,  & z_1,  & x_1 \oplus{} y_1) \\
\oplus & (x_2 \oplus{} y_2,& y_2 \oplus{} z_2,& x_2 \oplus{} y_2 \oplus{} z_2)\\
\oplus & (z_3,    &x_3 \oplus{} y_3,& y_3 \oplus{} z_3) \\
\oplus & (x_4 \oplus{} z_4,& x_4,& y_4)\\
\oplus & (y_5 \oplus{} z_5,& x_5 \oplus{} y_5 \oplus{} z_5,& x_5\oplus{}z_5)\\
\oplus & (x_6 \oplus{} y_6 \oplus{} z_6,& x_6 \oplus{} z_6,& x_6)
  \end{array}
\end{displaymath}

While {\em linear} erasure codes on $\ints_2$ exist, they do not achieve as high a signal-to-noise ratio.

\subsubsection{Transform cost}

A good matrix for the Combine step in EHC should be easy to evaluate.
At first glance, EHC might not look like it will reduce the number of multiplications needed, as the application of linear transformations usually requires multiplication.
However, since $T$ is not part of the randomness of the hash family, it can be designed to contain only values that are trivial to multiply by.

Ideally, a zero entry saves not just the multiplication but also an addition.
Barring that, entries with a small number of bits set (``low weight'') are easy to compute with a small number of additions, shifts, and subtractions.

The constraint in \cite{ehc-nandi} requires that any 3 columns of $T$ form an invertible matrix.
This is not as feasible in $\ints_{2^{64}}^{3 \times 9}$, as any such matrix will have at least one set of three columns with an even determinant, and which therefore has a non-trivial kernel.
The consequences on collision probability are discussed below.

HalftimeHash uses the $3 \times 9$ matrix below.
Applying this matrix to a vector of hash values requires 6 left shift operations and 18 additions.
There may be matrices of this size with fewer bits set in the entries -- it is easy to create a new instantiation of HalftimeHash with a new matrix should one be discovered.

\begin{displaymath}
  \left(
\begin{array}{rrrrrrrrr}
  0 & 0 & 1 & 4 & 1 & 1 & 2 & 2 & 1\\
  1 & 1 & 0 & 0 & 1 & 4 & 1 & 2 & 2\\
  1 & 4 & 1 & 1 & 0 & 0 & 2 & 1 & 2
\end{array}
\right)
\end{displaymath}

For $k \in \{2, 4, 5\}$, HalftimeHash uses

\[
\left(
\begin{array}{rrrrrrrrrrrr}
  1 & 0 & 1 & 1 & 1 & 2 & -1 & 2 & 1 & 1 & 4 & 1\\
  0 & 1 & 1 & -1 & 2 & 1 & 1 & -1 & 3 & 4 & 1 & 5
\end{array}
\right)
\]

\[
\left(
\begin{array}{rrrrrrrrrr}
  1 &  0 &   0&   0&   1&   1&  17&   2&   4& -17 \\
  0  & 1&   0  & 0&   1  & 2&   4  & 1&   3  & 4\\
  0 &  0 &  1 &  0 &  1 &  3 &  9 &  8 &  1 &  8\\
  0&   0  & 0&   1  & 1&   4 &-10&   4  &10&   3
\end{array}
\right)
\]

\[
\left(
\begin{array}{rrrrrrrrr}
 1 & 0 & 0 & 0 & 0 & 1 & 1 & 2 & 4\\
 0 & 1 & 0 & 0 & 0 & 1 & 2 & 1 & 7\\
 0 & 0 & 1 & 0 & 0 & 1 & 3 & 8 & 5\\
 0 & 0 & 0 & 1 & 0 & 1 & 4 & 9 & 8\\
 0 & 0 & 0 & 0 & 1 & 1 & 5 & 3 & 9
\end{array}
\right)
\]

%Furthermore, the input to each section is independent? Partially independent, based on the loss at the matrix multiplication step? Since each partition

\subsection{Tree hashing}

So far, we have only discussed hashing fixed-size blocks.
Carter \& Wegman outline, and Badger enhances, a simple tree-like construction of hashing to handle strings of different and arbitrary lengths. \cite{badger,carter-wegman-79}
The idea is direct composition of hash functions that take more bytes as input (excluding seeds) than they produce as output.
A more detailed description is in the overview of Section \ref{algo}.

For the tree hashing stage after EHC, three tree hashes run simultaneously (with independently-chosen seeds).
While this is just like the traditional NH approach HalftimeHash aims to improve upon, because the majority of the work of HalftimeHash is done in EHC, the higher levels of the tree are less consequential to the end-to-end time.

%These three hashes are run on the three output values of EHC.
%The collision probabilities on the outputs are independent (discussed below), and so three independent hashes with independent keys has the same effect 

% For HalftimeHash, $H_1$ is EHC and $H_i, i > 1$ are NH.



%% The three hash applications 

%% Let $H_1, H_2, \dots$ be $\varepsilon_i$-AU hash families, possibly identical, and let $S_1, S_2, \dots$ be their sets of seeds.
%% Each $H_i$ takes as input a seed from $S_i$ and a block of $d_i$ characters, producing as output blocks of $c_i < d_i$ characters.

%% To hash a string of length $n$, first treat it as a string of length $\lfloor n / d_i \rfloor$ blocks of $d_i$ characters each, with $n - d_i\lfloor n/d_i\rfloor$ remaining characters at the end, not in any block.
%% Randomly select an $s \in S_1$ and apply $H_1$ with $s$ to each block, concatenating the outputs, then appending the remaining $n-d_i\lfloor n/d_i\rfloor$ characters to that output.
%% This family takes as input a single seed from $S_1$ and a string $x$ of arbitrary length, $n$, and produces a string of length $(d_1 - c_1)\lfloor n / c_1 \rfloor + n$; we will call it $G_1$.

%% It is trivially extensible to all $H_i \mapsto G_i$.
%% Let $\widehat{G} \triangleq \bigcirc_i G_i$ be the composition of these, stopping when the output is small enough that the next $G_i$ cannot be applied.
%% The stopping point increases with string size.
%% Assume all $G_i$ share $c_1$ and $d_1$ and that $d_1 | c_1$
%% Then the longest string that only takes $m$ $G_i$'s has length $d_1 (c_1 / d_1)^m$.

%% For instance, if $c_1 = 2$ and $d_1 = 1$, then the result of hashing a string $x$ of length three is $H_2(s_2, H_1(s_1, x_1, x_2), x_3)$.
%% If $x$ has length four, the result is $H_2(s_2, H_1(s_1, x_1, x_2), H_1(s_1, x_3, x_4))$.
%% Note that the seed $s_1$ is re-used; this is discussed in more detail below.

% \subsubsection{Traversal strategies}

%% The repeated application of the $H_i$ outlined there can be viewed as a tree-like structure with branching factors $\lceil d_i / c_i \rceil$.
%% The input blocks are the leaves, and the final output is at the root.
%% As described above, this is an itererative process, starting from $G_1$ being applied to the entire input, then $G_2$ applied to the entire output of $G_1$, and so on.

As originally described in the literature, tree hash was an iterative process, started by applying $H_1$ to the entire input and buffering its output, applying $H_2$ to that, and so on.
This requires a scratch space linear in the size of the input.

Instead, traversal with a depth-first post-order strategy needs an amount of space only logarithmic in the size of the input.
A simplified recursive version might be written like

\begin{lstlisting}
uint32_t NH(uint32_t, uint32_t);

uint32_t TreeHash(uint32_t input[], size_t length) {
  if (1 == length) return input[0]
  if (2 == length) return NH(input[0], input[1]);
  size_t half_length = (length + 1) / 2;
  uint32_t lo = TreeHash(input, half_length);
  uint32_t hi = TreeHash(input + half_length,
                           length - half_length);
  return NH(lo, hi);
}
\end{lstlisting}

While this is direct and simple, the recursion produces slowdowns of up 10x or more over HalftimeHash's iterative depth-first post-order approach.
In this approach, a stack with height logarithmic in the size of the input is explicitly created to hold intermediate results; each level is large enough to store \texttt{fanout} Blocks.
To hash a string while traversing it, an input block is first hashed with EHC, with the output stored in the lowest level of the stack.
This continues until the lowest level of the stack is full (contains \texttt{fanout} Blocks).
When the next call of EHC is about to be made, HalftimeHash hashes the lowest level in the stack and puts it output in the next level of the stack.
This leaves room for the output of EHC in the lowest level.
A similar process happens further up the stack when higher levels are full.

The stack evolution is equivalent to iteration in a number system in which

\begin{itemize}
\item The empty sequence is a number; it represents the value $0$.
\item All non-empty sequences contain as their elements digits between $1$ and \texttt{fanout}.
\item The value of a sequence is just as in a \texttt{fanout}-ary number system, with the added redundancy of \texttt{fanout} being a permitted digit.
  For example, the number $(2,3)$ represents $2\cdot\texttt{fanout} + 3$, and $(1,0,0)$ represents the same number as $(\texttt{fanout},0)$.
\end{itemize}

%% One way to think about this is as a number system - each level represents a place, and the digits run from $1$ to \texttt{fanout}.
%% (Three Blocks count as ``1'', six as ``2'', and so on.)
%% Clearing a level is like ``carrying the 1'' in schoolbook arithmetic.

%% When DfsTreeHash returns, the stack contains data in triplets such that each element of a triplet is a hash of the same data as the other parts.
%% If these three parts are hashed into one new output, that increases the $\varepsilon$ to at least that of the new output.
%% Since HalftimeHash's main hash has $\varepsilon \geq 2^{-32}$, we must keep these three parts separate until the whole input has a small enough amount of data to process with functions with $\varepsilon$ around $2^{-64}$.



\subsection{Cleanup}

One the tree hash portion of HalftimeHash is complete, there still remains data to hash.

First, the tree hash leaves data in its stack - as much as $3bf$ words of data per level, where $b$ is the size of a SIMD register (in 64-bit words) and $f$ is \texttt{fanout}.

In addition to the data on the stack, there are $b$ characters that have yet to be hashed at all since HalftimeHash's EHC design expects to be fed $7 \cdot 3 = 21$ blocks of data at each invocation.

HalftimeHash uses three NH hash applications to complete the operation.
First, it feeds every hash value in the stack to the corresponding NH execution.
Second, it feeds every character in the remainder of the input string into {\em all three} NH executions.

HalftimeHash then returns three 64-bit words, for a total of 24 bytes of output.
This output is only AU, so users will likely want to rehash it (along with the input length) with a hash family with guarantees stronger than almost universality, such as tabulation hashing or SipHash. \cite{tabulation,siphash}
Post-cleanup tabulation hashing is available as an option in HalftimeHash's code; there is no noticable performance impact for strings longer than 1KB.


%% The stack itself could hold as many as \texttt{(fanout - 1) * 3 * sizeof(Block)} characters in each level in as many as $\lfloor \log_f (n/3B) - 1 \rfloor$ levels.
%% For a 128MiB file, $f = 8$, and $B = 64$, all of these combined could be as much as 8127 bytes!

%% \[64*22-1+ 7*3*64* \lfloor \log_8 (2^{27}/3/64) - 1 \rfloor
%% =
%% 1407 + 1344* \lfloor 5 \rfloor
%% =
%% 8127
%% \]

%% There are many potential mechanisms to finalize this data.
%% For a hash family with a goal of high throughput on {\em long} input strings, the importance of this part of the algorithm to total performance should be negligible.

%% HalftimeHash hashes this data by hashing all leftover values (the ones in the stack and the ones leftover in the input that were not read by the tree hash) with the NH hash function, with seeds not used in the creation of the stack.

%% The end result is three 64-bit values.
%% These, along with the length, constitute the output of HalftimeHash.

%% To reduce the size of the input, as well as to pass tests more stringent than the conditions defining almost-universal (or universe collapsing) hash families, HalftimeHash closes by using tabulation hashing to reduce the 256 bit value to 64 bits. \cite{tabulation}

\section{Analysis}
\label{analysis}

This section presents metrics of an execution of HalftimeHash, including the collision probability, number of multiplications performed, and the amount of input entropy used.
This section will treat HalftimeHash as abstract, rather than focusing on a single version with distance 3 in the erasure code.
For this we will use the following variables:

\begin {description}
\item[$w$] is the number of 64-bit words used in Encode step.
  In the main variant discussed above, this is three, as each of the seven inputs to EHC consists of three 64-bit words.
\item[$d$] is the {\em dimension} of the erasure code - the number of elements before applying the encoding, and $d e$ is the total number of 64-bit words fed into each EHC execution.
  In the main variant discussed above, $d$ is 7.
\item[$e$] is the {\em encoded size} - the number of 64-bit words in EHC after applying the encoding.
  In the main example above, this is nine.
\item[$k$] is the number of 64-bit words produced by the Combine step of EHC.
  This is also the number of 64-bit words produced at the end of the Cleanup step of HalftimeHash.
  For the main example discussed above, k is 3.
\item[$b$] the number of 64-bit words in a block.
  For AVX-512 variants of HalftimeHash, this is 8.
\item[$n$] is the string length in 64-bit words, padded at the word level to make $n$ integral.\footnote{This is used for analysis only; in the code string length is tracked in characters, without any padding.}
\item[$f$] is fanout, a user-adjustable integer parameter of at least 2 that can decrease the collision probability at the cost of increasing the amount of input entropy used.
  Fanout corresponds to the size of an instance fed to NH in the tree hash portion of HalftimeHash.
  In the code, HalftimeHash uses $f = 8$.
\item[$p$] is the max power of two that divides a determinant of any $k \times k$ matrix made from columns of the combining matrix $T$ in step three of EHC.
  Larger $p$ implies larger collision probability; doubling $p$ increases $\varepsilon$ by a factor of $2^k$.
\end{description}

HalftimeHash has the following steps that will be analyzed in order:

\begin{enumerate}
\item The parts of EHC:
  \begin {enumerate}
  \item Encode
  \item Hash
  \item Combine
  \end{enumerate}
\item Tree Hash
\item Cleanup
\item{} {\em [optional]} Finalize with tabulation hashing or SipHash
\end{enumerate}

A summary of the conclusions is in Table \ref{analysis-table}.

\subsection{EHC}

In each EHC execution, $dw$ words are read in.
They are converted to $ew$ words deterministically, without using any input entropy.
Those $ew$ words are then hashed with NH (the AU version) to $e$ words.
This uses $e(w - 1)$ words of input entropy and performs $e(w - 1)$ multiplications.

\begin{lemma}
  The EHC step of HalftimeHash is $2^{k(p-32)}$-AU.
\end{lemma}

\begin{proof}
As a reference and a warm-up, here is a way of viewing the corresponding proof from the original EHC paper:
in an environment in which the EHC matrix product is over a finite field, EHC is $2^{-32k}$-AU.
Given that $x$ and $y$ differ, let $\bar{x}$ and $\bar{y}$ be their encodings and $F$ be $k$ locations where $\bar{x}_i \neq \bar{y}_i$.
Let $W$ be the matrix formed by the columns of $T$ where the column index is in $F$.
We want to bound
\begin{equation}
  \label{ehc-delta}
  \Pr_s[W  H(s,\bar{x}) = W H(s,\bar{y})]
\end{equation}
where $H$ is the hash function used in step 2 of EHC.

Since the columns of $T$ are independent, $W$ is non-singular, and the equation is equivalent to $H(s,x) = H(s,y)$.
That is implied by
\[
\bigwedge_{i \in F} H(s_i, \bar{x}_i) = H(s_i, \bar{y}_i)
\]

Since the $s_i$ are all chosen independently, the probability of the conjunction is the product of the probabilities, showing
\[
\Pr_s[W H(s,\bar{x}) = W H(s,\bar{y})] \leq \varepsilon^k
\]

\paragraph{In HalftimeHash, this unravels at the reliance upon the trivial kernel of $W$.}

The columns of $T$ in HalftimeHash are linearly independent in $\ints$, so the matrix $W$ is non-singular in $\ints^{k\times{}k}$, but not necessarily non-singular in $\ints_{2^{64}}^{k\times{}k}$ because $\ints_{2^{64}}$ has zero divisors: every even number.

However, even in non-field rings like $\ints_{2^{64}}$, every matrix $A$ has a kind of pseudo-inverse $A^?$ such that $A \cdot A^? = A^? \cdot A = \det(A) I$.
This pseudo-inverse is just $\det(A)$ times the inverse of $A$ in $\rats^{k\times{}k}$.
Let $\det(W) = p2^q$, where $p$ is odd.
Now (\ref{ehc-delta}) reduces to
\[
\begin{array}{rl}
  &  \Pr_s[W H(s,x) = W H(s,y)]\\
  \leq & \Pr_s[W^?WH(s,x) = W^?WH(s,y)] \\
  = & \Pr_s[q2^pH(s,x) = q2^pH(s,y)] \\
  = & \Pr_s[2^pH(s,x) = 2^pH(s,y)] \\
  = & \Pr_s[H(s,x) \equiv H(s,y) \bmod 2^{64-p}] \\
  = & \Pr_s\left[\bigwedge_{i \in F} H(s_i,x_i) \equiv H(s_i,y_i) \bmod 2^{64-p}\right] \\
  = & \prod_{i \in F} \Pr_s\left[ H(s_i,x_i) \equiv H(s_i,y_i) \bmod 2^{64-p}\right] \\
  = & \left(2^p 2^{-32}\right)^{|F|} = 2^{k(p-32)}
\end{array}
\]
\end{proof}

\subsection{Tree hash}

\begin{lemma}
The tree hash portion of EHC produces $32k - k\lg\left\lfloor\log_f \lfloor n/dw\rfloor\right\rfloor$ bits of entropy.
\end{lemma}
\begin{proof}
Each of the $k$ hash trees is fed one word of input for every $dw$ words of input to EHC.
Every instance of $f$ words in the tree hash at level 0 is hashed with NH to produce a new word at level 1.
Inductively, if $m$ is the number of words fed into the tree hash, then $m \geq f^h$, where $h$ is the height of the tree.
Similarly, $m < f^{h+1}$, so $h = \lfloor \log_f m \rfloor$.
Combined with the classic result by Carter and Wegman of tree hashing's collision probability, the tree hash portion of HalftimeHash is AU with $\varepsilon = h 2^{-32}$.
Since $m = \lfloor n / d w \rfloor$, where $n$ is the length of the input in 64-bit words, the resulting collision probability overall is $\left(\left\lfloor \log_f \lfloor n / d w \rfloor \right\rfloor 2^{-32}\right)^k$
\end{proof}

The amount of entropy needed also corresponds to the height of the tree, $\left\lfloor\log_f \lfloor n/dw\rfloor\right\rfloor$, with $f - 1$ words needed for every level.
Here, HalftimeHash uses different seeds for the $k$ different trees to maintain independence of the entropy in the words of the output values, so the total number of 64-bit seeds used in the tree hash step is \[k(f-1)\left\lfloor\log_f \lfloor n/dw\rfloor\right\rfloor\]

In the limit the number of multiplications performed is nearly identical to the number of input words.
Of each $m$ words fed into the tree hash phase, $7/8$ths of them are multiplied by something in the first level.
Since each level is identical (except for seeds), this continues, approaching 1 from below as the sum of a geometric series.

\subsection{Cleanup}

There can be as many as as $f b h$ words of data in each of the $k$ stacks used for tree hashing.
For this data, HalftimeHash uses NH, which takes $f k b h$ words of entropy and just as many multiplications

There can also be as much as $b d w$ words of data in the raw input that are untouched so far by HalftimeHash.
Again, NH is used, but this time hashing $k$ times, since this data has not gone through EHC.
That requires $b d w k$ words of entropy and just as many multiplications.

For the untouched data, the number of words of entropy needed can be reduced by nearly a factor of $k$ using the Toeplitz construction. 
Let $r$ be the sequence of random words used in the leftover part of the cleanup.
Instead of using $[r_{ibdw}, r_{(i+1)bdw})$ as the keys to hash component $i$ with, HalftimeHash uses $[r_{i}, r_{bdw + i})$.
Both Nandi and Woelfel present short proofs that this construction for multi-part hash output is A$\Delta$U. \cite{ehc-nandi,woelfel-toeplitz}

%Instead of computing the $i$th component of the output using $b d w$ new words of entropy, the ith component 

\subsection{Cumulative analysis}

The combined collision probability is
\[2^{k(p-32)} + \left\lfloor \log_f \lfloor n / d w \rfloor \right\rfloor^k 2^{-32k} + 2^{-32k}\]
For the main example described above, with $k=3$, $p=2$, $f=8$, $d = 7$, and $w=3$, and for strings less than an exabyte in length, this is
\[2^{3(2-32)} + \left\lfloor \log_8 \lfloor 10^{18} /(8 \cdot 21) \rfloor \right\rfloor^3 2^{-96} + 2^{-96}\]
which is more than 83 bits of entropy.

The combined input entropy needed is

\[
e(w-1)
+ k(f-1)\left\lfloor\log_f \lfloor n/dw\rfloor\right\rfloor
+ f k b \left\lfloor\log_f \lfloor n/dw\rfloor\right\rfloor
+ b d w + k - 1
\]

When $b$ is 8, as in the AVX-512 version of HalftimeHash, this is less than 31KB for strings of length up to one exabyte -- 8.8KB for strings of length up to one megabyte.
6.1KB of this is for hashing the stack.

The number of multiplications is asymptotically driven by the EHC step

\[
en(w-1)/dw + nk/dw + f k b \left\lfloor\log_f \lfloor n/dw\rfloor\right\rfloor + b d w k
\]

\[
((e(w-1) + k)/dw)n + O(\log_f n)
\]

For a string of length 1MB, 84\% of the multiplications happen in the EHC step, and the number of multiplications is about one per ten bytes of input.
Intel's VTune tool show the same thing: 86\% of the clock cycles are spend in the EHC step.

\begin{table*}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
  \hline {\bf Step} & {\bf Mults (each)} & {\bf In Entropy (each)} & {\bf Out Entropy (each)} & {\bf Height} & {\bf Number} & {\bf Out words total}\\
  \hline EHC & $e(w-1)$ & $e(w-1)$ & $(32-p)k$ & 1 & $\lfloor n/dw \rfloor$ & $k \lfloor n/dw \rfloor$ \\
  \hline Tree Hash & $f-1$ & $f-1$ & $32k$ & $\left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ & $k \lfloor n / d w \rfloor$ / f & $f k \left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ \\
  \hline Cleanup stack & $b f k \left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ & $b f k \left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ & $32k$ & 1 & 1 & $k$ \\
  \hline Cleanup input & $b d k w$ &  $b d w + k - 1$ & $32k$ & 1 & 1 & $k$ \\
  \hline (Opt.) Tabulation & 0 &  $2^{11}k$ & $61 - \lg k$ & 1 & 1 & 1 \\
  \hline
\end{tabular}
\caption{\label{analysis-table}stats}
\end{table*}


%% \begin{tabular}{|l|c|c|c|c|c|c|}
%%   \hline {\bf Step} & {\bf Multiplications} & {\bf In Entropy} & {\bf Out Entropy} & {\bf Out words}\\
%%   \hline EHC & $e(w-1) \$ & $(32-p)k$ & 1 & $\lfloor n/dw \rfloor$ & $k \lfloor n/dw \rfloor$ \\
%%   \hline Tree Hash & $f-1$ & $32k$ & $\left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ & $(f-1) k \lfloor n / d w \rfloor$ & $f k \left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ \\
%%   \hline Cleanup stack & $b f k \left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ & $32k$ & 1 & 1 & $k$ \\
%%   \hline Cleanup input & $n / b d w$ & $32k$ & 1 & 1 & $k$ \\
%%   \hline
%% \end{tabular}



%% Finally, for the tabulation hashing of 32 bytes, \texttt{32 * 256 * 8 = 64KiB} of entropy is required.
%% This is the majority of all of the entropy used by HalftimeHash.
%% In entropy-constrained environments, users may wish to substitute finalizers that use less entropy, such as SipHash. \cite{siphash}

%% Overall the cleaup requires $8 f \lfloor \log_f (n/3B) - 1 \rfloor + 66 B + 65536$ bytes of entropy.

%% As for entropy provided, partially because of the long keys used in this section, the output before tabulation hashing is $(2^{-90} + 2^{-86} + 2^{-96})$-almost universal, with the $90$ coming from EHC and the $86$ coming from treehash, for a total entropy of over $85.9$ bits of entropy.
%% Tabulation adds $2^{-64}$, for a total of over $63.99$ bits of entropy.

%% After EHC, HalftimeHash essentially runs three treehashes.
%% Two input strings collide if they collide on all three treehashes.
%% Because the keys for each treehash were chosen indepenently, there is no dependence between a collision in one of the three parts and a collision in either of the other parts.

%% The collision probability of treehash is proportional to the product of the log of the input length and the collision probability of the basic hash function, which for HalftimeHash is NH.

%% If $w \neq x$, then $\Pr_s[H_i(s, w) = H_i(s, x)]$ is no more than $\varepsilon$.
%% This can be applied to each block without rasing $\varepsilon$:
%% If $[w, x] \neq [y, z]$, then either $w \neq y$, $x \neq z$, or both.
%% W.l.o.g, assume $w \neq y$; now if $G_i(s, [w, x]) = G_i(s, [y, z])$, then $H_i(s, w) = H_i(s, y)$, so \[Pr_s[G_i(s, [w, x]) = G_i(s, [y, z])] \leq \Pr_s[H_i(s, w) = H_i(s, y)] \leq \varepsilon\] and thus $G_i$ is $\varepsilon$-almost universal.
%% The extra appended characters do not alter this - the identity function is $0$-almost universal!

%% These $G_i$ can be applied repeatedly, starting with $G_1$ until the string remaining has fewer than $m$ characters.
%% At each level, $\varepsilon$ grows.
%% It is rather direct, and Carter \& Wegman show, that for any $U$ that is $\varepsilon_1$-almost universal and $V$ that is $\varepsilon_2$-almost universal, their composition $U \circ V$ is $(\varepsilon_1 + \varepsilon_2)$-almost universal.

%% The number of times $G_i$ needs to be applied is $\log_{m/j}(n/m) \pm O(1)$, so $\widehat{G} \triangleq  \bigcirc_i G_i$, which takes a seed from $\prod_i S_i$ and an input string of length $m$, is $\widehat{\varepsilon}$-almost universal, where $\widehat{\varepsilon} \triangleq (\log_{m/j}(n/m) \pm O(1)) \prod_i \varepsilon_i$.

%% This only applies to strings of length exactly $m$, but taking the output of $\widehat{G}$ and appending the value $m$ is a family that is $\widehat{\varepsilon}$-almost universal on strings of arbitrary length.

%% As shown in \cite{carter-wegman-79}, composing hash families like this incurs a linear extra cost: the composition of an $\varepsilon_1$-almost universal hash function and an $\varepsilon_2$-almost universal hash function is $(\varepsilon_1 + \varepsilon_2 - \varepsilon_1 \varepsilon_2)$-almost universal.
%% The number of functions composed depends on the height of the stack, which depends on the length of the string.

%% Each level in the stack/tree reduces the height of the string by a factor of \texttt{kFanout}.
%% Since a level needs \texttt{kFanout * 3 Blocks} before populating the next level, the total number of levels is
%% \[\lfloor \log_f n - \log_f (3fB) \rfloor = \lfloor \log_f (n/3B) - 1 \rfloor \]
%% where $f$ is \texttt{kFanout} and $B$ is \texttt{sizeof(Block)}.
%% For a string of 1TiB, $f = 8$, and $B = 64$, this is $10$.
%% Since the tree hash is essentially performing three almost-universal hashings with 32 bits of entropy per level, the total end entropy is $\lg (2^{32} / 10)^3 > 86$ bits.


%% The input seed must have enough entropy to hash all of the encoded values down from a three-tuple to a single value.
%% HalftimeHash hashes these almost-$\Delta$-universally using NH (see below).
%% This requires one 64-bit word for each input word, which is $9 \cdot 3 = 27$ based on $e$ and $i_w$.


%% Since the sums differ in at least one lcoation, this probability is at least

%% This is at least

%% \texttt{EhcUpperLayer} takes 1536 bytes as input and produces 256 bytes as output.
%% Physically these are each contiguous bytes in memory, but {\em logically}, the input should be thought of three {\em partitions}, each consisting \texttt{sizeof(Block) / sizeof(uint64\_t) = 8} interleaved {\em instances} of NH; each instance is broken up into \texttt{fanout} 64-bit {\em words}.
%% To hash an instance, \texttt{fanout - 1} words are hashed with NH and then added to the last word in the instance.
%% Thus each instance requires \texttt{fanout - 1} words of entropy, but they can all use the same entropy, since each can be thought of as a group of leaves in the treehash, with no overlapping groups.

%% Each instance has 32 bits of entropy and is essentially a portion of all the treehash edges at a given level.
%% As a whole, this family produces 32 bits of entropy.
%% However, the partitions will be essential to the bounds discussed below, so we need to dive deeper here.

%% We will see below that if the input to two different calls to \texttt{EhcUpperLayer} differ in one bit of input, they differ in all three partitions with high probability.
%% As such, the output of \texttt{EhcUpperLayer} differs for different input with probabilty $1 - ({2^{-32}})^3$.

%% \begin{table*}
%%   \begin{tabular}{|rr|c|c|c|}
%%     \hline
%%     &\bf{Step} & \bf{Entropy provided (bits)} & \bf{Entropy required (bytes)} & \bf{Output (bytes)}\\
%%     \hline \hline & EHC & 90 & 216 & $b k \lfloor n / b d i_w \rfloor$  \\
%%     \hline & Treehash & $96 - 3 \lg \lfloor \log_f(kn / di_w) \rfloor$ & $24f\lfloor\log_f(kn / di_w)\rfloor$ \\
%%     \hline \multirow{2}{*}{Cleanup} & stack & \multirow{2}{*}{96 (each and total)} & $3bf\lfloor \log_f(kn / di_w) \rfloor$ \\
%%     & remainder &  & $63b$ \\
%%     \hline \hline
%%     \multicolumn{4}{|c|}{\textit{\textbf{Generalized}}}\\
%%     \hline \hline
%%     & EHC & $k(33-k)$ & $8ei_w$ \\
%%     \hline &Treehash  & $32k - k\lg\lfloor \log_f(kn / di_w) \rfloor$ & $8fk\lfloor\log_f(kn / di_w)\rfloor$ \\
%%     \hline \multirow{2}{*}{Cleanup} & stack & \multirow{2}{*}{$32k$ (each and total)} & $bfk\lfloor \log_f(kn / di_w) \rfloor$ \\
%%     & remainder &  & $bdi_wk$ \\
%%     \hline
%%   \end{tabular}
%%   \caption{Notation: $d$ is the {\em dimension} of the erasure code and $e$ is the {\em block size}. For the main example in this paper, $(d, e) = (7, 9)$. $k$ is output width. For the codes in this paper, $k = e - d + 1$, which makes them {\em MDS codes}. The input width is $i_w$; this represents one measure (the other is $b$) of the size of the blocks of data fed into the encode and then hash steps of EHC. It is a user-tunable integer parameter of at least 1 that can increase performance by increased ILP. For codes like the $(7, 9)$ code that treat their input like 3-tuples, $i_w$ must be divisible by 3. $b$ is \texttt{sizeof(Block)}. $n$ is the string length in bytes. $f$ is fanout, a user-adjustable integer parameter of at least 2 that can increase the entropy provided at the cost of increasing the entropy required. }
%%   \label{entropy-table}
%% \end{table*}

%% \begin{table*}
%%   \begin{tabular}{|r|c|c|c|c|}
%%     \hline \bf{Step}     & \bf{input} & \bf{Entropic input} & \bf{output} & \bf{Entropic output} \\
%%     \hline
%%     \hline EHC, per word & $di_w$ & $ei_w$ & $k$ & $k(32 - p)$ \\
%%     \hline EHC, per block & $bdi_w$ & $ei_w$ & $bk$ & $k(32 - p)$ \\
%%     \hline EHC, per string & $\lfloor n/bdi_w \rfloor$ & $ei_w$ & $bk\lfloor n/bdi_w \rfloor$ & $k(32 - p)$ \\
%%     \hline Treehash, per partition-word (aka NH) & $f$ & $f-1$ & $1$ & $32$ \\
%%     \hline Treehash, per partition & $bf$ & $f-1$ & $b$ & $32$ \\
%%     \hline Treehash, per node & $bfk$ & $ k(f-1)$ & $bk$ &  $32 k$ \\
%%     \hline Treehash of height $h$ & $kbf^h \leq m \leq kb(f^h-1)/(f-1)$ & $hk(f-1)$ & $\leq bfhk$ & $k(32-h)$ \\
%%     \hline Cleanup stack per-partition & $bfh$ & $bfh$ & $1$ & $32$ \\
%%     \hline Cleanup stack & $bfhk$ & $bfhk$ & $k$ & $32k$ \\
%%     \hline Cleanup remainder & $bdi_w$ & $kbdi_w$ & $k$ & $32k$ \\
%%     \hline
%%   \end{tabular}
%%   \caption{
%%     The height of the tree, $h$, is proportional to the logairthm of the number of input words.
%%     The output of EHC is of size $bk\lfloor n/bdi_w \rfloor$.
%%     Each level of treehash reduces this size
%%     \\
%%     Notation: $d$ is the {\em dimension} of the erasure code and $e$ is the {\em block size}. For the main example in this paper, $(d, e) = (7, 9)$. $k$ is output width. For the codes in this paper, $k = e - d + 1$, which makes them {\em MDS codes}. The input width is $i_w$; this represents one measure (the other is $b$) of the size of the blocks of data fed into the encode and then hash steps of EHC. It is a user-tunable integer parameter of at least 1 that can increase performance by increased ILP. For codes like the $(7, 9)$ code that treat their input like 3-tuples, $i_w$ must be divisible by 3. $b$ is \texttt{sizeof(Block) / sizeof(uint64\_t)}. $n$ is the string length in bytes. $f$ is fanout, a user-adjustable integer parameter of at least 2 that can increase the entropy provided at the cost of increasing the entropy required.
%%     $p$ is the max power of two that divides a determinant of a $kxk$ matrix in $V$.\\
%%     s\\
%%   d}
%% \end{table*}

%% Figure \ref{entropy-table} generalizes the entropy requirements and guarantees of HalftimeHash.
%% For the $(7,9)$ code discussed above, the experiments below have been configured to use $f = 8$, $n \leq 10^6$, and $B = 64$.
%% For these parameters, the maximum entropy used is $216 + 65*64\lfloor\log_8(10^6/192)\rfloor + 63*64 \leq$ 21KiB and the minimum entropy produced is
%% \[-\lg\left(2^{-90} + 2^{-96}\lfloor\log_8(10^6/192)\rfloor^3 + 2^{-96}\right)\]
%% \[= 88.99 \textrm{ bits}.\]

%% Increasing the maximum string length to 1 terabyte ($10^{12}$ bytes) decreases the entropy produced to $85.9$ bits while increasing the entropy required to $45.8$KiB.

%   \includegraphics[width=8cm]{benchmark-output/{c5a.l-medium}.eps}

%gnuplot> set xlabel 'input length in bytes'; set ylabel 'output bits of entropy'; set title 'HalftimeHash192 entropy'; unset key; set grid; set logscale x; unset logscale y; set yrange [85:91]; set xrange[1.0:1000000000000.0]; plot -log(2**-96 + 2**-90 + (2**-96)*( floor(log((x < 192 ? 192 : x)/192)/log(8))**3))/log(2)

% gnuplot> set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set title 'c5a.l'; set key top right; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[0:50]; set xrange[1000:*]; plot 'c5a.large-clang-11-f4ab1fb-13107.txt' using 1:(1.0/ column(2)) lw 0.5,  for [i=3:3] 'c5a.large-clang-11-f4ab1fb-13107.txt' using 1:(1.0/ column(i)) lw 0.5, for [i=4:4] 'c5a.large-clang-11-f4ab1fb-13107.txt' using 1:(1.0/ column(i)) lw 0.5, for [i=5:5] 'c5a.large-clang-11-f4ab1fb-13107.txt' using 1:(1.0/ column(i)) lw 0.5, for [i=6:6] 'c5a.large-gcc-10-f4ab1fb-13107.txt' using 1:(1.0/ column(i)) lw 0.5, for [i=7:7] 'c5a.large-gcc-10-f4ab1fb-13107.txt' using 1:(1.0/ column(i)) lw 0.5, for [i=8:8] 'c5a.large-clang-11-f4ab1fb-13107.txt' using 1:(1.0/ column(i)) lw 0.5, for [i=9:9] 'c5a.large-clang-11-f4ab1fb-13107.txt' using 1:(1.0/ column(i)) lw 0.5


% gnuplot> set xrange[1.0:1000000000000.0]; plot 216 + 65 * 64 * ((x < 192) ? 0 : floor(log(x/192) / log(8))) + 63 * 64

% gnuplot> set yrange [85:91]; set xrange[1.0:1000000000000.0]; plot -log(2**-96 + 2**-90 + (2**-96)*( floor(log((x < 192 ? 192 : x)/192)/log(8))**3))/log(2)

% gnuplot> set key top right; set yrange [*:*]; set xrange[1.0:1000000000000.0]; plot -log(2**-64 + 2**-60 + (2**-64)*( floor(log((x < 64 ? 64 : x)/64)/log(8))**2))/log(2)


\section{Tests}




\includegraphics[width=8cm]{speed-v-epsilon.eps}
\captionof{figure}{
  \label{frontier}
  Trade-offs for almost-universal string hashing functions on strings of size 250KB.
  The lower left corner is ideal: low execution time and low collision probability.
  Note that the clhash and UMASH executions are not along the efficient frontier / layer-of-maxima, in that one version of HalftimeHash is always faster and has lower collision probability.
  This contrasts with the case of short strings, where clhash and UMASH have better performance.
}

\includegraphics[width=8cm]{gcc-local-hh4.eps}
\captionof{figure}{
  Different output widths of HalftimeHash.
  Note that even the longest of these is faster than clhash and UMASH; see Figure \ref{frontier}.
}

\begin{figure*}
\begin{tabular}{cccc}
\includegraphics[width=9cm]{line-cl-hh24.eps}
&
\includegraphics[width=9cm]{amd-cl-hh24.eps}
\end{tabular}
\caption{
  Comparison of Intel (i7-7800x) and AMD (EC2 c5a.large, 7R32 chip) performance.
  In both cases, for long strings, HalftimeHash with 24 bytes of output is faster than clhash and UMASH.
}
\end{figure*}

\begin{table*}
\begin{tabular}{cccc}
\includegraphics[width=9cm]{amd-16.eps}
&
\includegraphics[width=9cm]{amd-24.eps}
\end{tabular}
\caption{AMD performance.
  AMD chips do not have AVX-512 instructions, so blocks of that size are simulated with two AVX2 registers.
  Using a block size of 256 bits performs better.}
\end{table*}


% gnuplot> set terminal postscript eps enhanced color size 5,5 fontfile "/usr/share/texlive/texmf-dist/fonts/type1/public/libertine/LinLibertineOB.pfb" "LinLibertineOB,29"; set output 'speed-v-epsilon.eps'; set xlabel 'nanoseconds per byte'; set ylabel 'collision probability'; set title "Speed vs. collision (lower left is better)"; unset key; set logscale y 2; unset logscale x; set grid; set yrange[1e-50:1e-10]; set xrange[0.0:0.08]; unset offsets; set xrange[0.0:0.08]; set format y '2^{%L}'; plot 'points-example.txt' using 1:2:3 with labels point pt 7 offset char 0,1

TODO: compare to VHASH

HalftimeHash24, when composed with SipHash or tabulation hashing as a final step, passes all SMHasher tests.
This is faster in SMhasher's bulk speed tests than all other previously tested hash function, excluding \texttt{crc32\_pclmul\_le\_16}, a deterministic 32-bit hash function implemented in assembly in the Linux kernel.

TODO: Notes on SMHasher and relatives

%% \includegraphics[width=8cm]{benchmark-output/m5zn-medium.eps}
%% \captionof{figure}{HalftimeHash's performance on m5zn EC2 nodes compared to other AU families. Performance cliffs correspond to L1 and L2 cache sizes.}

%% \includegraphics[width=8cm]{benchmark-output/entropy-bpb.eps}
%% \caption{The output entropy provided by the main HalftimeHash variant presented in this text.
%%   The output size of this variant is 192 bits (or 256, including the input length).
%%   For strings up to a terabyte long, the entropy doesn't fall below 85 bits.
%% }

%% \includegraphics[width=8cm]{benchmark-output/{c5a.l-medium}.eps}
%% \captionof{figure}{AMD performance.
%%   Using one AVX2 register as the block size is more efficient than using two as an ad-hoc AVX512 register.
%%   However, no matter which Block size is chosen, HalftimeHash is faster than the two CLMUL-based hash families.
%% }

% gnuplot> set terminal postscript eps enhanced color size 5,4 fontfile "/usr/share/texlive/texmf-dist/fonts/type1/public/libertine/LinLibertineOB.pfb" "LinLibertineOB,29"; set output 'speed-v-epsilon.eps'; set xlabel 'nanos per byte'; set ylabel 'collision probability'; set title "Speed vs. collision (lower left is better)"; unset key; set logscale y; unset logscale x; set grid; set yrange[1e-40:1e-10]; set xrange[0.0:0.08]; unset offsets; set xrange[0.0:0.08]; plot 'points-example.txt' using 1:2:3 with labels point pt 7 offset char 0,1


% gnuplot> unset label; set label "63.99 bits" at 2,2.51 left font "LinLibertineO,30"; set xtics ("KB" 1000, "MB" 1e6, "GB" 1e9, "TB" 1e12, "PB" 1e15, "EB" 1e18); set terminal postscript eps enhanced color size 5,7 fontfile "/usr/share/texlive/texmf-dist/fonts/type1/public/libertine/LinLibertineOB.pfb" "LinLibertineOB,29"; set output 'entropy-bpb.eps'; set key top right; set xlabel 'input length in bytes'; set ylabel 'output entropy bits per byte of output'; set title 'Bits of entropy per output byte'; set grid; set logscale x; unset logscale y; set yrange [*:3]; set xrange[1.0:10e17]; plot -log(2**-128 + 2**-116 + (2**-128)*(floor(log((x < 256 ? 256 : x)/256)/log(8))**4))/log(2)/40 lw 8 title 'HalftimeHash40', -log(2**-96 + 2**-90 + (2**-96)*( floor(log((x < 192 ? 192 : x)/192)/log(8))**3))/log(2)/32 lw 8 title 'HalftimeHash32', -log(2**-64 + 2**-60 + (2**-64)*(floor(log((x < 128 ? 128 : x)/128)/log(8))**2))/log(2)/24 lw 8 title 'HalftimeHash24'


In addition to the version of HalftimeHash that produces 192-bits of output, we have also implemented ones that produce 128 and 256 bits of output.

TODO: explain block size differences better

% TODO: do this on EC2 and in separate runs for each family
%TODO: do a ratio test and do it over the whole range
%\begin{figure}
%   \includegraphics[width=8cm]{benchmark-output/i7-7800x-large.eps}
%  \captionof{figure}{i7-7800x for large files. HalftimeHash's advantage shrinks as the string to be hashed no longer fits in L3 cache.}
%\end{figure}

For each machine configuration, we show the graph of the best performing compiler for each hash family, as defined by the throughput on input strings of length 250KB.

%% \begin{figure}
%%   \includegraphics[width=8cm]{benchmark-output/local-small.eps}
%%   \caption{HalftimeHash is designed for string longer than 1KiB.
%%     Here we see its performance on stirngs up to 10KB in size.
%%     Below 1KiB, CLHASH and UMASH dominate.
%%     A more general purpose using HalftimeHash would replace it with a different hash function, piecewise, on short strings.
%%     This is already in-line with existing hash functions.
%%     For instance, CLHASH uses straight NH, without polynomial hashing, for strings under 1KiB in size.
%%   }
%% \end{figure}


%% \begin{figure*}
%%    \includegraphics[width=16cm]{benchmark-output/m5zn-small.eps}
%%   \caption{EC2 node for small files. HalftimeHash performs poorly.}
%% \end{figure*}


% gnuplot> set terminal postscript eps enhanced color size 4,4 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'i7-7800x-large.eps'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set title 'i7-7800x'; set key top right; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[0:*]; set xrange[*:10010000]; plot 'past-all-l3.txt' using 1:(1.0/ column(4)) with lines lw 5 title 'Halftime128', '' using 1:(1.0/ column(3)) with lines lw 5 title 'Halftime192', '' using 1:(1.0/ column(2)) with lines lw 5 title 'Halftime256', '' using 1:(1.0/ column(6)) with lines lw 5 title 'CLHASH', '' using 1:(1.0/ column(5)) with lines lw 7 title 'UMASH',


% gnuplot> set terminal postscript eps enhanced color size 6,6 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'i7-7800x-large.eps'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set title 'i7-7800x'; set key top right; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[0:*]; set xrange[*:*]; plot 'past-all-l3.txt' using 1:(1.0/ column(4)) title 'Halftime128', '' using 1:(1.0/ column(3)) title 'Halftime192', '' using 1:(1.0/ column(2)) title 'Halftime256', '' using 1:(1.0/ column(6)) title 'CLHASH', '' using 1:(1.0/ column(5))   title 'UMASH',


% gnuplot> set terminal postscript eps enhanced color size 6,3.67 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'm5zn-small.eps'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set title 'm5zn.l'; set key top left; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:*]; set xrange[1:1000]; plot 'm5zn.large-gcc-10-4049690-256.txt' using 1:(1.0/ column(2)) title 'Halftime128', 'm5zn.large-gcc-10-4049690-16.txt' using 1:(1.0/ column(2)) title 'Halftime192', 'm5zn.large-gcc-10-4049690-1.txt' using 1:(1.0/ column(2)) title 'Halftime256', 'm5zn.large-gcc-10-4049690-8192.txt' using 1:(1.0/ column(2)) title 'CLHASH', 'm5zn.large-clang-11-4049690-4096.txt' using 1:(1.0/ column(2))   title 'UMASH',


%% \begin{figure}
%%   \includegraphics[width=8cm]{benchmark-output/m5zn-medium.eps}
%%   \caption{HalftimeHash is designed for string longer than 1KiB.
%%     Here we see its performance on stirngs up to 10KB in size.
%%     Below 1KiB, CLHASH and UMASH dominate.
%%     A more general purpose using HalftimeHash would replace it with a different hash function, piecewise, on short strings.
%%     This is already in-line with existing hash functions.
%%     For instance, CLHASH uses straight NH, without polynomial hashing, for strings under 1KiB in size.
%%   }
%% \end{figure}

% gnuplot> set terminal postscript eps enhanced color size 3.67,6 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'm5zn-small.eps'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set title 'm5zn.l'; set key top left; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:*]; set xrange[1:1000]; plot 'm5zn.large-gcc-10-4049690-256.txt' using 1:(1.0/ column(2)) title 'Halftime128', 'm5zn.large-gcc-10-4049690-16.txt' using 1:(1.0/ column(2)) title 'Halftime192', 'm5zn.large-gcc-10-4049690-1.txt' using 1:(1.0/ column(2)) title 'Halftime256', 'm5zn.large-gcc-10-4049690-8192.txt' using 1:(1.0/ column(2)) title 'CLHASH', 'm5zn.large-clang-11-4049690-4096.txt' using 1:(1.0/ column(2))   title 'UMASH',


%gnuplot> set terminal postscript eps enhanced color size 3.67,5 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'm5zn-medium.eps'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; unset title; set key top left; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[20:100]; set xrange[1000:*]; plot 'm5zn.large-gcc-10-4049690-256.txt' using 1:(1.0/ column(2)) title 'HalftimeHash24', 'm5zn.large-gcc-10-4049690-16.txt' using 1:(1.0/ column(2)) title 'HalftimeHash32', 'm5zn.large-gcc-10-4049690-1.txt' using 1:(1.0/ column(2)) title 'HalftimeHash40', 'm5zn.large-gcc-10-4049690-8192.txt' using 1:(1.0/ column(2)) title 'CLHASH', 'm5zn.large-clang-11-4049690-4096.txt' using 1:(1.0/ column(2)) title 'UMASH',

% gnuplot> set terminal postscript eps enhanced color size 3.67,6 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'm5zn-medium.eps'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set title 'm5zn.l'; set key top left; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[20:100]; set xrange[1000:*]; plot 'm5zn.large-gcc-10-4049690-256.txt' using 1:(1.0/ column(2)) title 'Halftime128', 'm5zn.large-gcc-10-4049690-16.txt' using 1:(1.0/ column(2)) title 'Halftime192', 'm5zn.large-gcc-10-4049690-1.txt' using 1:(1.0/ column(2)) title 'Halftime256', 'm5zn.large-gcc-10-4049690-8192.txt' using 1:(1.0/ column(2)) title 'CLHASH', 'm5zn.large-clang-11-4049690-4096.txt' using 1:(1.0/ column(2))   title 'UMASH',


% gnuplot> set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set title 'm5zn.l'; set term wxt background rgb "#ffffff"; set key top left; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[20:100]; set xrange[1000:*]; plot 'm5zn.large-gcc-10-4049690-256.txt' using 1:(1.0/ column(2)) title 'Halftime128', 'm5zn.large-gcc-10-4049690-16.txt' using 1:(1.0/ column(2)) title 'Halftime192', 'm5zn.large-gcc-10-4049690-1.txt' using 1:(1.0/ column(2)) title 'Halftime256', 'm5zn.large-gcc-10-4049690-8192.txt' using 1:(1.0/ column(2)) title 'CLHASH', 'm5zn.large-clang-11-4049690-4096.txt' using 1:(1.0/ column(2))   title 'UMASH',


% gnuplot> set term wxt background rgb "#ffffff"; set key top left; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[0.1:100]; set xrange[1000:*]; plot 'm5zn.large-gcc-10-4049690-256.txt' using 1:(1.0/ column(2)) with lines lw 2, 'm5zn.large-gcc-10-4049690-16.txt' using 1:(1.0/ column(2)) with lines lw 2, 'm5zn.large-gcc-10-4049690-1.txt' using 1:(1.0/ column(2)) with lines lw 2, 'm5zn.large-gcc-10-4049690-8192.txt' using 1:(1.0/ column(2)) with lines lw 2, 'm5zn.large-clang-11-4049690-4096.txt' using 1:(1.0/ column(2)) with lines lw 2,

% gnuplot> set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set title 'm5zn.l'; set term wxt background rgb "#ffffff"; set key top left; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[20:100]; set xrange[1000:*]; plot 'm5zn.large-gcc-10-4049690-256.txt' using 1:(1.0/ column(2)) with lines lw 2 title 'Halftime128', 'm5zn.large-gcc-10-4049690-16.txt' using 1:(1.0/ column(2)) with lines lw 2 title 'Halftime192', 'm5zn.large-gcc-10-4049690-1.txt' using 1:(1.0/ column(2)) with lines lw 2 title 'Halftime256', 'm5zn.large-gcc-10-4049690-8192.txt' using 1:(1.0/ column(2)) with lines lw 2 title 'CLHASH', 'm5zn.large-clang-11-4049690-4096.txt' using 1:(1.0/ column(2)) with lines lt rgb '#3434ff' lw 2 title 'UMASH',



%% \includegraphics{benchmark-output/vs_clhash.eps}

%% \includegraphics{benchmark-output/systematic/c5a-l.eps}

%% \includegraphics{benchmark-output/systematic/inf1-xl.eps}

%% \includegraphics{benchmark-output/systematic/m5a-l.eps}

%% \includegraphics{benchmark-output/systematic/t3-m.eps}

%% \includegraphics{benchmark-output/systematic/z1d-l.eps}

%% \begin{verbatim}
%% set terminal postscript eps enhanced color size 6.5,3.67 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'vs_clhash.eps'; set title'i7-7800x';
%% set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:85]; set xrange[*:*];
%% plot './clhash-0.1-percent-still-100-for-paper.txt'using 1: (1.0/ column(2)) with lines, './halftime-line-badger-0.1-percent-still-100.txt'using 1: (1.0/ column(2)) with lines,

%% set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:45]; set xrange[*:*]; plot for [i=2:7] './c5a.l.clang.txt' using 1: (1.0/ column(i)) with lines, for [i=2:7] './c5a.l.gcc.txt' using 1: (1.0/ column(i)) with lines,

%% set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:45]; set xrange[*:*]; plot './c5a.l.clang.txt' using 1: (1.0/ column(3)) with lines, '' using 1: (1.0/ column(7)) with lines,

%% set terminal postscript eps enhanced color size 6.0,3.67 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'c5a-l.eps'; set title 'c5a-l'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:85]; set xrange[*:*]; plot './c5a.l.clang.txt' using 1: (1.0/ column(7)) with lines, '' using 1: (1.0/ column(3)) with lines,

%% set terminal postscript eps enhanced color size 6.0,3.67 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'inf1-xl.eps'; set title 'inf1-xl'; 
%% set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:75]; set xrange[*:*]; 
%% plot for [i=7:7] './inf1.xl.gcc.txt' using 1: (1.0/ column(i)) with lines, for [i=3:3] './inf1.xl.clang.txt' using 1: (1.0/ column(i)) with lines

%% gnuplot> set terminal postscript eps enhanced color size 6.0,3.67 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'm5a-l.eps'; set title 'm5a-l'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:75]; set xrange[*:*]; plot for [i=7:7] './m5a.l.gcc.txt' using 1: (1.0/ column(i)) with lines, for [i=5:5] './m5a.l.clang.txt' using 1: (1.0/ column(i)) with lines

%% gnuplot> set terminal postscript eps enhanced color size 6.0,3.67 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 't3-m.eps'; set title 't3-m'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[*:75]; set xrange[*:*]; plot for [i=7:7] './t3.m.gcc.txt' using 1: (1.0/ column(i)) with lines, for [i=2:2] './t3.m.clang.txt' using 1: (1.0/ column(i)) with lines

%% gnuplot> set term wxt background rgb "#bacada"; set style fill transparent solid 0.01 noborder; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[0:85]; set xrange[1000:*]; plot  './t3.m.clang.txt' using 1:(1.0/ column(2)):(0.2 * column(1)) with lines lw 5 lt rgb "white", './t3.m.gcc.txt' using 1:(1.0/ column(7)):(0.2 * column(1)) with lines lw 1 lt rgb "#880000"

%% gnuplot> set term wxt background rgb "#ffffff"; set style fill transparent solid 0.01 noborder; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[0:85]; set xrange[1000:*]; plot  './t3.m.clang.txt' using 1:(1.0/ column(2)):(0.2 * column(1)) with lines lw 2 lt rgb "#ca0020", './t3.m.gcc.txt' using 1:(1.0/ column(7)):(0.2 * column(1)) with lines lw 2 lt rgb "#2020ca"

%% gnuplot> set style fill transparent solid 0.1 noborder; set term wxt background rgb "#ffffff"; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[0:85]; set xrange[1000:1000000]; plot './t3.m.clang.txt' using 1:(1.0/ column(2)):(500 * log(column(1))):(1) with ellipses, './t3.m.gcc.txt' using 1:(1.0/ column(7)):(500 * log(column(1))):(1) with ellipses

%% gnuplot> set style fill transparent solid 1 noborder; set term wxt background rgb "#ffffff"; set key autotitle columnheader; unset logscale y; set logscale x; set grid; set yrange[0:85]; set xrange[1:*]; plot '/tmp/precise-gcc-all.txt' using 1:(1.0/ column(2)) with lines lw 6 lt rgb '#3434ff', '' using 1:(1.0/column(3)):(0.05 * column(1)) with circles lt rgb '#000000', '' using 1:(1.0/column(4)) with lines lw 2 lt rgb '#ff3434'

%% gnuplot> set terminal postscript eps enhanced color size 6.0,3.67 fontfile "/usr/share/texmf/fonts/type1/public/cm-super/sfbx1200.pfb" "SFBX1200,20"; set output 'local-small.eps'; set title 'local-small'; set xlabel 'input length in bytes'; set ylabel 'billion bytes per second'; set key autotitle columnheader; unset logscale y; set logscale x; set grid;  set key top left; set yrange[*:*]; set xrange[*:*]; plot for [i in "2 6 10 14 15"] 'gcc10-4049690-all.txt' using 1:(1.0/ column(0+i)) title columnhead(i) lw 1

%% \end{verbatim}

TODO: CAVP @ NIST

TODO: crypto community tests, eBASH

\section{Future work}

HalftimeHash is a fast family for strings that are more than 1KB but still fit in cache.
The introduction pondered combining HalftimeHash with hash families designed for shorter strings.
This remains future work.

Additionally, HalftimeHash depends on platforms supporting multiplication of two 32-bit integers that produces a 64-bit integer.
This operation is present in many environments, but not all.
Notable exceptions include some embedded platforms and native JavaScript, which only support 53-bit integers.

TODO: longer outputs to match clhash or poly1305

TODO: compare against optimized poly1305 from kernel

TODO: test crc crc32\_pclmul\_le\_16 and see if that can go more than twice as fast as HalftimeHash, including making it work on unaligned data

\bibliographystyle{ACM-Reference-Format}
\bibliography{library}

%% \appendix
%% \section{Additional utilities}

%% An additional block type is a repetition of multiple blocks of smaller size.
%% This has utility, when, for instance, the same hash function must be used on different machines with different available SIMD instructions.


%% The remainder describes the user-facing functions, all of which wrap the \texttt{Hash()} function above.

%% %

\end{document}
\endinput
%% The last equation is equivalent to $H(s,x) \equiv H(s,y) 2^{64 - p}$, with operations in $\ints^k$, rather than $\ints_{2^{64}}^k$.
%% This is equivalent to $\exists 0 \leq r < 2^p, 2^p(H(s,x) - H(s,y)) = \delta' + 2^{64} r$.
%% Since these operations are pointwise on three components, there are not just $2^p$ values, but $(2^p)^3$ values of $r$, so by the union bound we have
%% \[
%% \max_{\delta' \in \Phi^3}, \Pr_s[2^p(H(s,x) - H(s,y)) = \delta'] \leq (2^r\varepsilon)^3
%% \]
%% Since $r=2$ and $\varepsilon = 2^{-32}$, this is $2^{-90}$.


%% and has an inverse $W^{-1}$, so this equation is equivalent to $H(s,x) - H(s,y) = W^{-1} \cdot \delta$.
%% \begin{equation}
%%   \begin{array}{rl}
%%   \label{ehc-delta-final}
%%   &\max_{\delta \in \Phi^3}, \Pr_s[H(s,x) - H(s,y) = W^{-1} \cdot \delta] \\
%%   \leq&   \max_{\delta' \in \Phi^3}, \Pr_s[H(s,x) - H(s,y) = \delta']
%% \end{array}
%% \end{equation}

%% By \ref{ehc-h-delta}, this is $\leq \varepsilon^3$.


%% Note that this bound is not tight.
%% In particular, when $\delta'$ is not dividible by $2^p$, $\max_{\delta' \in \Phi^3}, \Pr_s[2^p(H(s,x) - H(s,y)) = \delta'] = 0$


%% Let $H$ be the hash function in step 1 of EHC and $G$ be the full EHC hash family, including all three steps.
%% The output size of EHC is three 64-bit words with the property that if $x \neq y$ are input strings and $\delta$ is a three-tuple in $\ints_{2^{64}}^3$, then
%% \[
%% \forall_{i < 3}, \Pr_s[G(s,x)_i = G(s,y)_i + \delta_i] = 2^{-32}
%% \]
%% and
%% \[
%% \Pr_s\left[\forall_{i < 3}, G(s,x)_i = G(s,y)_i + \delta_i\right] = 2^{-96}
%% \]
%% In other words, $G$ is $2^{-32}$-almost-$\Delta$ universal and the $3$ output values of $G$ are independent.

%% First, the proof of independence:

%% Nandi's proof of this relies on the invertability of non-singular matrices to preserve entropy:
%% Let $w, z$ be two unequal input sequences from $\Sigma^m$.
%% The encoding produces $x, y$ of length $m + 2$ with minimum distance three.
%% That is, $\exists i < j < k \leq m + 2, x[i] \neq y[i] \wedge x[j] \neq y[j] \wedge x[k] \neq y[k]$.

%% The hash functions are picked independently from an $\varepsilon$-almost-$\Delta$ universal family $H \in S \to \Sigma \to \Phi$, so
%% \begin{equation}
%%   \label{ehc-h-delta}
%%   \forall \delta \in \Phi^3, \Pr_s[H(s,x) = H(s,y) + \delta] \leq \varepsilon^3
%% \end{equation}
%% Individually, each component the three components in the equation is true with probability $\varepsilon$, but we need the fact that if $r \neq t$ then for all $z$, $H(s_r, z_r)$ and $H(s_t, z_t)$ are independent to get the $\varepsilon^3$ bound.


%% That's how much entropy is provided.
%% The entropy required is three 64-bit keys per column, since we collapse the three-part input into one part before applying the combination transform.
%% That's a total of \texttt{3 * 9 * sizeof(uint64\_t) = 216} bytes.

%% \[
%% \forall_{i < 3}, \Pr_s[G(s,x)_i = G(s,y)_i + \delta_i] = 2^{-32}
%% \]

%% Since $x$ and $y$ differ, their encodings differ in at least three places.
%% The matrix used in the Combine step has no more than two zeros in a row.\footnote{In fact, any matrix in which all sets of three columns are non-singular have this property.}
%% This means there is at least one non-zero column in row $i$ where $x$ and $y$ differ before the hashing step.
%% After the NH hashing step, each location still differs with probability $1-2^{-32}$.


%% % TODO: does this work better if at most one zero in each column?

%% \[
%% \Pr_s \left[\sum_j V_{i,j}  H(s_j, e(x)) = \sum_j V_{i,j}  H(s_j, e(y)) + \delta\right]
%% \]

%% Wlog, let $m$ be a non-zero location in $V_i$ where $e(x)$ and $e(y)$ differ.
%% The above probability now can be converted to

%% \[
%% \Pr_s \left[V_{i,m} H(s_m, e(x)) =  V_{i,m} H(s_m, e(y)) + \delta'\right]
%% \]

%% Where $\delta' = \sum_{j\neq m} V_{i,j}  (H(s_j, e(y)) - H(s_j, e(x))) + \delta$.
%% Let $V_{i,m} = p2^q$, where $p$ is odd.
%% The probability above is now equivalent to

%% \[
%% \Pr_s \left[2^q H(s_m, e(x)) =  2^q H(s_m, e(y)) + \delta''\right]
%% \]

%% where $\delta'' = q^{-1}\delta'$.
%% This is possible because $q$ has an inverse in $\ints_{2^{64}}$.
%% This is identical to the analysis above regarding even numbers and inverses, and the probability is no more than $2^{q-32}$.
%% Since $q$ is at most 2 in the V we use

%% By the almost-$\Delta$ universality of $H$, $\Pr_s \left[V_{i,m} H(s_m, e(x)) =  V_{i,m} H(s_m, e(y)) + \delta'\right]$

%%  LocalWords:  HalftimeHash codomain ISA's UMASH falkhash MeowHash
%%  LocalWords:  MetroHash FarmHash clhash wyhash farmhash UHASH EHC
%%  LocalWords:  VHASH Nandi's ISA strided Wegman TODO VTune NH's XXH
%%  LocalWords:  UMAC VMAC carryless fanout Nandi Gabrielyan Toeplitz
%%  LocalWords:  Gabrielyan's HalftimeHash's Woelfel SMHasher
