\documentclass[sigconf, nonacm]{acmart}

 %% \documentclass[acmsmall, nonacm]{acmart}

 %% \geometry{twoside=false,
 %%   includeheadfoot, head=13pt, foot=2pc,
 %%   paperwidth=11in, paperheight=8.5in,
 %%   top=58pt, bottom=44pt, inner=46pt, outer=46pt,
 %%   marginparwidth=2pc,heightrounded
 %% }%


%\usepackage[margin=1in]{geometry}
%\usepackage{accsupp}
%\usepackage{moreverb}
\usepackage{amsfonts}
\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{color}
%\usepackage{comment}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{microtype}
\usepackage{multirow}
%\usepackage{newpxtext}
%\usepackage{verbatim}
%\usepackage{cleveref}


%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{http://vldb.org/pvldb/format_vol14.html}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 

\lstloadlanguages{C++}

\lstset{%
  basicstyle=\ttfamily,%\small,
  language=C++,
%  breaklines=true,
  columns=fullflexible,
%  identifierstyle=\color{red},
  showstringspaces=false,
  commentstyle=\color[rgb]{0.5,0,0}\itshape,
  %backgroundcolor=\color[rgb]{0.92,0.92,1}
}

%\pagenumbering{gobble}
%\pagestyle{empty}

\newtheorem{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{claim}{Claim}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\rats}{\mathbb{Q}}
\newcommand{\nats}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\cplx}{\mathbb{C}}

%% \lstset{basicstyle=\ttfamily,
%% escapeinside={||},
%% mathescape=true}

\newenvironment{blockquote}
{\begin{quote}\itshape}
{\end{quote}}
\title{HalftimeHash: modern hashing without 64-bit multipliers or finite fields}
\author{Jim Apple}
\orcid{0000-0002-8685-9451}
\email{jbapple@apache.org}

%\hypersetup{draft}

\begin{document}
%\thispagestyle{fancyplain}
%\thispagestyle{empty}

\begin{abstract}
HalftimeHash is a new algorithm for hashing long strings.
The goals are few collisions (different inputs that produce identical output hash values), high performance, and portability -- HalftimeHash should be easy to implement correctly on new ISA's and easy to code in almost any programming language.

Compared to the fastest universal hash functions on long strings, clhash and UMASH, HalftimeHash decreases collision probability while also increasing performance by over 50\%.
\end{abstract}

\maketitle

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
\endgroup
}
%%% VLDB block end %%%

\section{Introduction}
A hash family is a map from a set of seeds $S$ and a domain $D$ to a codomain $C$.\footnote{Seeds are sometimes called {\em salts} or {\em keys}. Given that elements of the domain of hash functions are also sometimes called {\em keys}, this work will avoid this term altogether, for the sake of clarity.}
A hash family $H$ is called is $\varepsilon$-almost universal (``$\varepsilon$-AU'' or just ``AU''\footnote{Technically, all hash families are 1-AU. In this work, a function will be called AU only if it is $o(1)$-AU.}) when
\[\forall x,y \in D, x \neq y \implies \mathrm{Pr}_{s \in S}[H(s, x) = H(s, y)] \leq \varepsilon\]
The intuition behind this definition is that the input to be hashed might not be random, but collisions can still be made unlikely by picking randomly from a hash {\em family}, rather than anchoring on a specific hash {\em function} that maps from $D$ to $C$ but does not take a seed as an input.

HalftimeHash is a new hash family designed for

\begin{enumerate}
\item High speed on long strings.
  Combining HalftimeHash with a hash family that is designed with short strings in mind can yield a hash family that performs better than either one in isolation. \cite{siphash,umash}
\item A simple implementation that is easily portable between programming languages and machine ISA's.
\item Tunable output length for applications that require it, such as one-time authorization. \cite{nacl}
\end{enumerate}

For HalftimeHash, the family parameters are:

\begin{description}
\item[Domain $D$] consists of all strings of size less than $2^{64}$ bytes.
\item[Codomain $C$] is 16, 24, 32, or 40 bytes.
  These are easily expanded to larger sizes.
\item[Collision probability $\varepsilon$] is less than than $2^{-83}$ bits for input shorter than one exabyte (for the family producing 24 bytes of output), with shorter strings producing lower collision probability.
\item[Seeds $S$] are larger for longer strings in the domain, but are less than 34KB for strings as long as a exabyte.
\end{description}

HalftimeHash is a ``universe collapsing'' function, designed to hash long strings into short ones. \cite{linear-hash-functions,hashing-without-primes-revisited,cuckoo-journal}
This differs from short-input families like SipHash, suitable for hashing short strings down to 64 bits, or general purpose families like UMASH, suitable for hashing strings of any length down to 64 bits.
Universe collapsing functions are especially suitable for pairing with short-input families: when $n$ items are to be handled by a hash-based algorithm, even if they are very long, a universe-collapsing function that reduces them to hash values of length $c \lg n$ bits for some suitable $c \geq 3$ produces no collisions with high probability.
Short-input/short-output families can then be used on those $c \lg n$ bits. \cite{universe-collapse-linear-probing,siphash,tabulation,simple-hash-functions-work}

\subsection{Prior work}

Hash family invention is the pursuit of a significant hobbyist community, yielding a number of fast hash algorithms running at rates exceeding 8 bytes per cycle on modern x86-64 processors, including Fast Positive Hash, falk\-hash, xxh, Meow\-Hash, and UMASH, and cl\-hash. \cite{smhasher}
Of these, only cl\-hash and U\-MASH include claims of being AU.

Other previous work on AU hashing long strings is no longer as competitive, performance-wise, as it once was, including UHASH and VHASH.
UHASH was designed specifically for SIMD speed, but it predates Nandi's breakthrough EHC algorithm as used in HalftimeHash and hard-codes the SIMD register width. \cite{umac,ehc-nandi}
VHASH similarly came about before EHC and is an update to UHASH designed to take advantage of 64-bit architectures, and performs much better than UHASH, but still lags compared to the current fastest hash families. \cite{vmac,smhasher}

\subsection{Results}

HalftimeHash uses less than 1000 lines of code in C++, does not use any assembly, and makes spare use of the C++ standard library.
It can take advantage of vector ISA extensions like AVX-512, AVX2, and SSE, but it also runs on the C++ virtual machine model with no architecture-specific features.
It is a parameterized algorithm that supports straightforward creation of new versions with larger SIMD register sizes or longer output hash values.
On strings longer than 1KB, HalftimeHash is typically 55\% faster than clhash, the AU hash family that comes closest in performance to HalftimeHash.
It also surpasses the performance of other high-performance string hashing algorithms that don't have AU guarantees such as Fast Positive Hash, falkhash, xxh3, and MeowHash.
% TODO: graph this perf comparison with other non-universal families


\includegraphics[width=8cm]{smhasher-speed.eps}
\captionof{figure}{
  \label{smhasher-speed}
    Some variants of HalftimeHash are faster than the fastest hash families in the SMHasher suite on a CPU with AVX-512 instructions, even families that come with no AU guarantees. \protect\cite{smhasher}
    MeowHash128 and xxh128 produce 128 output bits, all other SMHasher functions produce 64 bits.
    HalftimeHash16 produces 128 output bits, HalftimeHash24 produces 192 output bits, HalftimeHash32 produces 256 bits, and HalftimeHash40 produces 320 bits.
    Of the families here, only HalftimeHash and xxh128 pass all SMHasher tests.
}



\section{Notations and Conventions}

$H$ and $G$ will denote hash families.
Generally, capital letters will denote sets or functions, including matrices.
Lowercase letters will usually represent strings or non-negative integers.
$n$ generally denotes the size of the input string, usually in 64-bit words.
Strings are sometimes instead treated as sequences of bytes or of 32-bit words.
Arithmetic operations may be over elements of $\ints_m$, $\ints$, or sometimes $\rats$.

``32-bit multiplication'' will mean multiplying two unsigned 32-bit words and producing a single 64-bit word.
``64-bit multiplication'' similarly refers to the operation producing a 128-bit product.
All machine integers are unsigned.

Subscripts are used to indicate a numbered component of a sequence, numbered from 0.

$\varepsilon$ is called the {\em collision probability} of $H$; it is inversely related to $H$'s {\em output entropy}, $-\lg \varepsilon$.
By convention, the entropy of a hash family with collision probability of $0$ is $+\infty$.
(An example of a hash family like this is the identity function variant $(s, x) \mapsto x$.)

HalftimeHash takes two arguments, the {\em seed} and the {\em input}.
The seed is sometimes referred as {\em input entropy}, which is distinguished from $-\lg \varepsilon$ both because it is an explicit part of the input and because it is measured in words or bytes, not bits.

Each step of HalftimeHash applies various transforms to groups of input values.
These groups are called {\em instances}.
Instances are logically contiguous but physically strided, for the purpose of making SIMD vector processing faster and simpler.
The number of 64-bit words between two 64-bit words in the same instance, plus one (for a fencepost), is the expected SIMD parallelism.
This physically contiguous set is called a {\em block}; the number of 64-bit words in a block is called the {\em block size}.
The processing of a transform on a single instance is called an {\em execution}.
Because instances are logically contiguous, when possible, the text will elide references to the block size.


HalftimeHash produces output that is collision resistant among strings of the same length.
Adding collision resistance between strings of {\em different} lengths to such a hash family requires only appending the length at the end of the output.
This turns, for instance, a hash family that produces 24 bytes of output into a hash family that produces 32 bytes of output.

The main portion of the text describes a particular instantiation of HalftimeHash that produces 24 bytes of output.
This will be generalized in Section \ref{analysis}.
Variants will be specified by their number of output bytes: HalftimeHash16, HalftimeHash24, HalftimeHash32, or HalftimeHash40.

Except where otherwise mentioned, all benchmarks were run on an Intel i7-7800x (a Skylake chip that supports AVX512), running Ubuntu 18.04, with clang++ 11.0.1.

%-++20201218093139+43ff75f2c3fe-1$\sim$exp1$\sim$20201218203800.157

%For each machine configuration, we show the graph of the best performing compiler for each hash family, as defined by the throughput on input strings of length 250KB.

\section{Algorithm}
\label{algo}

\subsection{Overview}

The macrostructure of HalftimeHash is in the style of tree hash, as described by Carter and Wegman. \cite{carter-wegman-79}
The leaves of a tree are the words of the input string, while the root is the output hash value.
Each level of edges is represented by a single hash function chosen at random from an AU family.

Algebraically, a tree hash family consists of an alphabet $\Sigma$ of {\em blocks}, an almost-universal hash family $H$, and a sequence of seeds $s$ for that family.
The codomain of $H$ is $\Sigma^k$, and the codomain is $\Sigma$.
Each execution of a tree hash family is defined recursively.
An execution of height $1$ is a tuple consisting of an input value, a seed, and an output value for $H$.
An execution of height $n+1$ is $k$ applications of level $n$ along with the output value that is the result of applying $H$ with seed $s_{n+1}$ to the output values of the $k$ applications of level $n$.

More concretely, $H(s_1,\langle{}a,b\rangle) = x$ is a tree hash execution of height 1, $H(s_1, \langle H(s_0, \langle a,b\rangle), H(s_0, \langle c,d\rangle)) = y$ is a tree hash execution of height 2, and so on.
Note that the seed $s_0$ is used for both executions of height 1.

Carter and Wegman show that if $H$ is $\varepsilon$-AU, then a tree of height $m$ is $m\varepsilon$-AU.
%The Badger construction (\cite{badger}) shows how to manage strings where there is a mismatch in the input and output arity at some levels without negatively impacting the universality.

A novel implementation choice in HalftimeHash is using an erasure coding of the input in the leaves.
The central idea is called ``EHC'', for {\em Encode, Hash, Combine}. \cite{ehc-nandi}
The application of the erasure code adds ``minimum distance'' $d$, so that any two input values that differ in any location produce encoded outputs that differ in at least $d > 1$ locations.
This decreases the probability of collisions by a factor of $\varepsilon^{d-1}$.

\includegraphics[width=8cm]{Diagram2.eps}
\captionof{figure}{
  \label{tree}
  A simplified example of the tree hash shape used in HalftimeHash.
  At the leaves are the input blocks to be hashed.
  Each instance of seven words is initially encoded and then hashed down to three blocks in a process called ``EHC''. \cite{ehc-nandi}
  Following that, three copies of Carter-Wegman-style tree hash run on each of the components produced during the EHC step.
  Note that the right-most blocks bypass one round of hashing.
  This is equivalent to hashing with the identity function, which has $\varepsilon = 0$, and so allows the values to pass-through without adding any extra collision probability. \cite{badger}
}

This section focuses on a particular instantiation of this formula with distance $3$, producing a hash value $24$ bytes in size.
Adding variants with different distances is direct, and the benchmarking results below include variants with distance 2, 4, and 5.

\subsection{The NH hashing primitive}

The hashing primitive used in HalftimeHash to hash small, fixed-length sequences is called {\em NH}. \cite{umac}
NH's domain is the same as its set of seeds: sequences with an even number of 32-bit unsigned integers.
The codomain is 64 bit integers.
The definition of NH is:

\[
\sum_{i=0}^m (d_{2i} + s_{2i})(d_{2i+1} + s_{2i+1})
\]

The $d + s$ additions are in the ring $\ints_{2^{32}}$, while all other operations are in the ring $\ints_{2^{64}}$.

Unusually, NH has only $32$ bits of output entropy; typically universal families with $64$ bits of output have nearly $64$ bits of entropy. \cite{umash,clhash}
With NH, typically one of a number of other algorithms is used in a final additional step to reduce the size of the output to $32$ bits.
Creating additional output entropy bits is usually done by running the algorithm twice. \cite{umash,umac}
Despite this potential extra hashing (depending on needed entropy), NH has been widely used in hash families such as UMAC \cite{umac}, VMAC \cite{vmac}, clhash \cite{clhash}, UMASH \cite{umash}, XXH3 \cite{xxh3}, and more, usually with $k = 64$.

In HalftimeHash, no 64-bit-mul\-ti\-pli\-ca\-tions are needed.
This is in support of two goals --
the first is portability to platforms or programming languages without native widening unsigned 64-bot multiplications.
Languages like Java, Python, and Swift can do these long multiplications, but not without calling out to C or slipping into arbitrary-precision-integer code, the latter of which decreases performance substantially.
Many programming languages do not include a way to perform native carryless (aka polynomial) multiplication, which is the central operation in the fastest AU hash functions other than HalftimeHash. \cite{umash,clhash}

% TODO: check this

The other reason HalftimeHash avoids 64-bit multiplications is SIMD-friendliness.
The x86-64 ISA extensions SSE2, AVX2, and AVX-512F all contain instruction to simultaneously multiply multiple pairs of 32-bit words, producing multiple 64-bit values.
Aarch64 has similar instructions in the NEON set.
% The POWER ISA also contains this, but only on 128 bits at once?

%% HalftimeHash uses both NH as described above and a weakened version of NH:

%% \[
%% d_0 + 2^{32} d1 + \sum_{i=1}^m (d_{2i} + s_{2i})(d_{2i+1} + s_{2i+1})
%% \]

%% This version is still $2^{-32}$-AU, but it loses a condition called {\em$ \varepsilon$-almost $\Delta$-universal} ($\varepsilon$-A$\Delta$U).
%% A$\Delta$U is strictly stronger then AU, but is not needed in every part of HalftimeHash.
%% Section \ref{analysis} discusses this in more detail.

%The default is 8, saving $12.5\%$ of the erstwhile $n/8 \pm O(1)$ multiplications to hash strings of length $n$.

%Now we can build the hash function referenced above, which takes a Block of values and a Block of entropy, spitting out a hashed Atom that can later be collapse via %addition to yield the almost-delta-universal NH family.

\subsection{Encode-Hash-Combine}

At the leaves of the tree, HalftimeHash uses an algorithm from Nandi called ``EHC'' for {\em encode-hash-combine}: \cite{ehc-nandi}

\begin{enumerate}
\item A sequence of items is encoded with an erasure code with {\em minimum distance $d$}.
  In this context, ``distance'' means the Hamming distance -- the number of unequal items between two sequences.
  An encoding with minimum distance $d$ means that all sequences that are unequal in {\em any} place before encoding are unequal in at least $d$ places after encoding.
  For example, encoding $(a,b)$ as $(a,b,a+b)$ has minimum distance 2.
\item Each item in the encoded sequence is hashed, with an independently chosen seed, to a field.
  HalftimeHash will use NH here. % (the AU variant, not the A$\Delta$U one)
\item The resulting sequence of hash values is put through a linear transformation on that field, $T$.
  $T$ must have the property that any $d$ columns of it are independent, and so a square matrix made up of any $d$ columns is injective.
%  A linear transformation with this property is said to be {\em d-MDS}.
\end{enumerate}

%% Nandi proves that if the hash family used in step 2 is $\varepsilon$-A$\Delta$U, then the output of step 3 is $\varepsilon^d$-A$\Delta$U.
%% In contrast, to avoid slow operations over finite fields, HalftimeHash uses a non-linear erasure code and a linear transformation $T$ with a looser constraint on the invertability; see below.

%% Additionally, the other components of HalftimeHash do not depend on EHC being A$\Delta$U, only AU.
%% Just as in NH, this saves HalftimeHash a percentage of multiplications.
%% See Section \ref{analysis} for more details.

\subsubsection{Erasure coding}

One key aspect of the efficiency of HalftimeHash is that the additional size added by encoding the input string is low as a percentage of the total hashed data.
For every 7 words that are fed into step 1 of EHC, 9 words emerge from step 2.
The lower that ratio ($9/7$) is, the fewer multiplications that HalftimeHash does.
For example, if HalftimeHash used a different erasure code that had minimum distance 2 over 14 words, and the output of step 2 were 17 words, then the additional multiplications induced by encoding the data would be lower.
%By the Singleton bound, A code with minimum distance $d$ must be at least $d-1$ characters longer than the pre-encoded data.
However, in order to use large blocks, the matrix $T$ must grow as well.
For finite fields, this is not an issue, but in $\ints_{2^{64}}$, as discussed below, it becomes a constraint satisfaction problem to find matrices that preserve entropy.
%While this work can be done offline, it's not obvious the effort will yield good results.

%% Instead of using large blocks, HalftimeHash uses a short erasure code but uses SIMD to maintain high throughput.
%% All operations extend pointwise, and HalftimeHash hashes eight different parts of the string simultaneously.\footnote{Fewer than eight are interleaved for narrower SIMD registers like AVX2's 256-bit registers.}

HalftimeHash uses a non-linear erasure code discovered by Gab\-ri\-el\-yan. \cite{9-7-erasure-code}
It maps seven 3-tuples to nine 3-tuples with a minimum distance of three.
Once the encoding is done, the hash function in step 2 reduces the size of the universe, by hashing each 3-tuple to a single block.

Gabrielyan's code produces 2 additional 3-tuples without altering the seven inputs.\footnote{This is called a ``systematic'' code.}
The first additional tuple is just the \texttt{xor} of the seven input words.
The second tuple is the following sum, numbering the input words as $(x_0, y_0, z_0), (x_1, y_1, z_1), \dots$.

\begin{displaymath}
  \begin{array}{rlcr}
       & (x_0, & y_0, & z_0)\\
\oplus & (y_1,  & z_1,  & x_1 \oplus{} y_1) \\
\oplus & (x_2 \oplus{} y_2,& y_2 \oplus{} z_2,& x_2 \oplus{} y_2 \oplus{} z_2)\\
\oplus & (z_3,    &x_3 \oplus{} y_3,& y_3 \oplus{} z_3) \\
\oplus & (x_4 \oplus{} z_4,& x_4,& y_4)\\
\oplus & (y_5 \oplus{} z_5,& x_5 \oplus{} y_5 \oplus{} z_5,& x_5\oplus{}z_5)\\
\oplus & (x_6 \oplus{} y_6 \oplus{} z_6,& x_6 \oplus{} z_6,& x_6)
  \end{array}
\end{displaymath}

While {\em linear} erasure codes on $\ints_2$ exist, they do not achieve as high a signal-to-noise ratio. \cite{codetables.de}

\subsubsection{Transform cost}

At first glance, EHC might not look like it will reduce the number of multiplications needed, as the application of linear transformations usually requires multiplication.
However, since $T$ is not part of the randomness of the hash family, it can be designed to contain only values that are trivial to multiply by.

Ideally, a zero entry saves not just the multiplication but also an addition.
Barring that, entries with a small number of bits set (``low weight'') are easy to compute with a small number of additions, shifts, and subtractions.

The constraint in \cite{ehc-nandi} requires that any 3 columns of $T$ form an invertible matrix.
This is not as feasible in $\ints_{2^{64}}^{3 \times 9}$, as any such matrix will have at least one set of three columns with an even determinant, and which therefore has a non-trivial kernel.
Instead, HalftimeHash uses a matrix selected so that the largest power of 2 that divides any determinant is $2^2$.
The consequences on collision probability are discussed in Section \ref{analysis}.

HalftimeHash uses the $3 \times 9$ matrix below.
Applying this matrix to a vector of hash values requires 6 shift operations and 18 additions.
There may be matrices of this size with fewer bits set in the entries -- it is easy to create a new instantiation of HalftimeHash with a new matrix should one be discovered.

\begin{displaymath}
  \left(
\begin{array}{rrrrrrrrr}
  0 & 0 & 1 & 4 & 1 & 1 & 2 & 2 & 1\\
  1 & 1 & 0 & 0 & 1 & 4 & 1 & 2 & 2\\
  1 & 4 & 1 & 1 & 0 & 0 & 2 & 1 & 2
\end{array}
\right)
\end{displaymath}

For $k \in \{2, 4, 5\}$, HalftimeHash uses

\[
\left(
\begin{array}{rrrrrrrrrrrr}
  1 & 0 & 1 & 1 & 2 & 1 & 4\\
  0 & 1 & 1 & 2 & 1 & 4 & 1
\end{array}
\right)
\]

\[
\left(
\begin{array}{rrrrrrrrrr}
 0 & 0 & 0 & 1 & 1 & 4 & 2 & 4 & 1 & 1 \\
 0 & 1 & 2 & 0 & 0 & 1 & 1 & 2 & 4 & 1 \\
 2 & 0 & 1 & 0 & 4 & 0 & 1 & 1 & 1 & 1 \\
 1 & 1 & 0 & 1 & 0 & 0 & 4 & 1 & 2 & 8
\end{array}
\right)
\]

\[
\left(
\begin{array}{rrrrrrrrr}
 1 & 0 & 0 & 0 & 0 & 1 & 1 & 2 & 4\\
 0 & 1 & 0 & 0 & 0 & 1 & 2 & 1 & 7\\
 0 & 0 & 1 & 0 & 0 & 1 & 3 & 8 & 5\\
 0 & 0 & 0 & 1 & 0 & 1 & 4 & 9 & 8\\
 0 & 0 & 0 & 0 & 1 & 1 & 5 & 3 & 9
\end{array}
\right)
\]

%Furthermore, the input to each section is independent? Partially independent, based on the loss at the matrix multiplication step? Since each partition

\subsection{Tree hashing}

As presented, NH and EHC only hash fixed-size blocks.
Carter and Wegman outline a simple tree-like construction of hashing to handle strings of different and arbitrary lengths. \cite{badger,carter-wegman-79}
The idea is direct composition of hash functions that take more bytes as input (excluding seeds) than they produce as output.
A more detailed description is in the overview of Section \ref{algo} and in Figure \ref{tree}.

For the tree hashing stage after EHC, three tree hashes run simultaneously (with independently-chosen seeds).
While this is just like the traditional NH approach HalftimeHash aims to improve upon, because the majority of the work of HalftimeHash is done in EHC, the higher levels of the tree are less consequential to the end-to-end time.

%These three hashes are run on the three output values of EHC.
%The collision probabilities on the outputs are independent (discussed below), and so three independent hashes with independent keys has the same effect 

% For HalftimeHash, $H_1$ is EHC and $H_i, i > 1$ are NH.



%% The three hash applications 

%% Let $H_1, H_2, \dots$ be $\varepsilon_i$-AU hash families, possibly identical, and let $S_1, S_2, \dots$ be their sets of seeds.
%% Each $H_i$ takes as input a seed from $S_i$ and a block of $d_i$ characters, producing as output blocks of $c_i < d_i$ characters.

%% To hash a string of length $n$, first treat it as a string of length $\lfloor n / d_i \rfloor$ blocks of $d_i$ characters each, with $n - d_i\lfloor n/d_i\rfloor$ remaining characters at the end, not in any block.
%% Randomly select an $s \in S_1$ and apply $H_1$ with $s$ to each block, concatenating the outputs, then appending the remaining $n-d_i\lfloor n/d_i\rfloor$ characters to that output.
%% This family takes as input a single seed from $S_1$ and a string $x$ of arbitrary length, $n$, and produces a string of length $(d_1 - c_1)\lfloor n / c_1 \rfloor + n$; we will call it $G_1$.

%% It is trivially extensible to all $H_i \mapsto G_i$.
%% Let $\widehat{G} \triangleq \bigcirc_i G_i$ be the composition of these, stopping when the output is small enough that the next $G_i$ cannot be applied.
%% The stopping point increases with string size.
%% Assume all $G_i$ share $c_1$ and $d_1$ and that $d_1 | c_1$
%% Then the longest string that only takes $m$ $G_i$'s has length $d_1 (c_1 / d_1)^m$.

%% For instance, if $c_1 = 2$ and $d_1 = 1$, then the result of hashing a string $x$ of length three is $H_2(s_2, H_1(s_1, x_1, x_2), x_3)$.
%% If $x$ has length four, the result is $H_2(s_2, H_1(s_1, x_1, x_2), H_1(s_1, x_3, x_4))$.
%% Note that the seed $s_1$ is re-used; this is discussed in more detail below.

% \subsubsection{Traversal strategies}

%% The repeated application of the $H_i$ outlined there can be viewed as a tree-like structure with branching factors $\lceil d_i / c_i \rceil$.
%% The input blocks are the leaves, and the final output is at the root.
%% As described above, this is an itererative process, starting from $G_1$ being applied to the entire input, then $G_2$ applied to the entire output of $G_1$, and so on.

As originally described in the literature, tree hash was an iterative process, started by applying $H$ to the entire input with $s_0$ and buffering its output, applying $H$ with $s_1$ to that, and so on.
This requires a scratch space linear in the size of the input.

Instead, traversal with a depth-first post-order strategy needs an amount of space only logarithmic in the size of the input.
A simplified recursive version might be written like

\begin{lstlisting}
uint32_t NH(uint32_t, uint32_t);

uint32_t TreeHash(uint32_t input[], size_t length) {
  if (1 == length) return input[0]
  if (2 == length) return NH(input[0], input[1]);
  size_t half_length = (length + 1) / 2;
  uint32_t lo = TreeHash(input, half_length);
  uint32_t hi = TreeHash(input + half_length,
                           length - half_length);
  return NH(lo, hi);
}
\end{lstlisting}

While this is direct and simple, the recursion produces slowdowns of up 10x over HalftimeHash's iterative depth-first post-order approach.
In this approach, a stack with height logarithmic in the size of the input is explicitly created to hold intermediate results; each level is large enough to store some user-tunable number of Blocks, $f$.
To hash a string while traversing it, an input block is first hashed with EHC, with the output stored in the lowest level of the stack.
This continues until the lowest level of the stack is full (contains $f$ Blocks).
When the next call of EHC is about to be made, HalftimeHash hashes the lowest level in the stack with AU NH and puts it output in the next level of the stack.
This compaction process leaves room for the output of EHC in the lowest level.
A similar process happens further up the stack when higher levels are full.\footnote{This is nearly identical to the way log-structured merge-trees work. \cite{lsm-survey}}.

The stack evolution is equivalent to iteration in a redundant zeroless number system in base $f$; for examples of the application of number systems to data structures, see \cite{redundant-zeroless}.

%% \begin{itemize}
%% \item The empty sequence is a number; it represents the value $0$.
%% \item All non-empty sequences contain as their elements digits between $1$ and \texttt{fanout}.
%% \item The value of a sequence is just as in a \texttt{fanout}-ary number system, with the added redundancy of \texttt{fanout} being a permitted digit.
%%   For example, the number $(2,3)$ represents $2\cdot\texttt{fanout} + 3$, and $(1,0,0)$ represents the same number as $(\texttt{fanout},0)$.
%% \end{itemize}

%% One way to think about this is as a number system - each level represents a place, and the digits run from $1$ to \texttt{fanout}.
%% (Three Blocks count as ``1'', six as ``2'', and so on.)
%% Clearing a level is like ``carrying the 1'' in schoolbook arithmetic.

%% When DfsTreeHash returns, the stack contains data in triplets such that each element of a triplet is a hash of the same data as the other parts.
%% If these three parts are hashed into one new output, that increases the $\varepsilon$ to at least that of the new output.
%% Since HalftimeHash's main hash has $\varepsilon \geq 2^{-32}$, we must keep these three parts separate until the whole input has a small enough amount of data to process with functions with $\varepsilon$ around $2^{-64}$.



\subsection{Sweep}

% TODO: try pass through to reduce the stack size and they sweep key size

Once the tree hash portion of HalftimeHash is complete, there still remains data to hash.

First, the tree hash leaves data in its stack - as much as $bf$ words of data per level per tree, where $b$ is the size of a SIMD register (in 64-bit words) and $f$ is \texttt{fanout}.

In addition to the data on the stack, there are some characters that have yet to be hashed at all since HalftimeHash's EHC design expects to be fed $7 \cdot 3 = 21$ words at each invocation, and no fewer, so there are words remaining at the end of the input string.

HalftimeHash uses three NH hash applications to complete the operation.
First, it feeds every hash value in the three stacks to the corresponding NH execution.
Second, it feeds every character in the remainder of the input string into {\em all three} NH executions.
%% For both, HalftimeHash uses the A$\Delta$U version of NH.

HalftimeHash then returns the three NH sums, for a total of 24 bytes of output.
This output is only AU, so users will likely want to rehash it (along with the input length) with a hash family with guarantees stronger than almost universality, such as tabulation hashing or SipHash. \cite{tabulation,siphash}
Post-sweep tabulation hashing is available as an option in HalftimeHash's code; there is no noticeable performance impact for strings longer than 1KB.


%% The stack itself could hold as many as \texttt{(fanout - 1) * 3 * sizeof(Block)} characters in each level in as many as $\lfloor \log_f (n/3B) - 1 \rfloor$ levels.
%% For a 128MiB file, $f = 8$, and $B = 64$, all of these combined could be as much as 8127 bytes!

%% \[64*22-1+ 7*3*64* \lfloor \log_8 (2^{27}/3/64) - 1 \rfloor
%% =
%% 1407 + 1344* \lfloor 5 \rfloor
%% =
%% 8127
%% \]

%% There are many potential mechanisms to finalize this data.
%% For a hash family with a goal of high throughput on {\em long} input strings, the importance of this part of the algorithm to total performance should be negligible.

%% HalftimeHash hashes this data by hashing all leftover values (the ones in the stack and the ones leftover in the input that were not read by the tree hash) with the NH hash function, with seeds not used in the creation of the stack.

%% The end result is three 64-bit values.
%% These, along with the length, constitute the output of HalftimeHash.

%% To reduce the size of the input, as well as to pass tests more stringent than the conditions defining almost-universal (or universe collapsing) hash families, HalftimeHash closes by using tabulation hashing to reduce the 256 bit value to 64 bits. \cite{tabulation}

\section{Analysis}
\label{analysis}

This section presents metrics of an execution of HalftimeHash, including the collision probability, number of multiplications performed, and the amount of input entropy used.
This section will treat HalftimeHash as abstract, rather than focusing on a single version with distance 3 in the erasure code.
For this we will use the following variables:

\begin {description}
\item[$w$] is the number of blocks used in Encode step.
  In the main variant discussed above, this is three, as each of the seven inputs to EHC consists of three blocks.
\item[$d$] is the {\em dimension} of the erasure code - the number of elements before applying the encoding, and $d w$ is the total number of blocks fed into each EHC execution.
  In the main variant discussed above, $d$ is 7.
\item[$e$] is the {\em encoded size} - the number of blocks in EHC after applying the encoding.
  In the main example above, this is nine.
\item[$k$] is the number of blocks produced by the Combine step of EHC.
  This is also the number of 64-bit words produced at the end of the Sweep step of HalftimeHash.
  For the main example discussed above, k is 3.
\item[$b$] the number of 64-bit words in a block.
  For AVX-512 variants of HalftimeHash, this is 8.
\item[$n$] is the string length in 64-bit words, padded at the word level to make $n$ integral.\footnote{This is used for analysis only; in the code string length is tracked in characters, without any padding.}
\item[$f$] is fanout, a user-adjustable integer parameter of at least 2 that can decrease the collision probability at the cost of increasing the amount of input entropy used.
  Fanout corresponds to the size of an instance fed to NH in the tree hash portion of HalftimeHash.
  In the code, HalftimeHash uses $f = 8$ by default.
\item[$p$] is the maximum power of 2 that divides a determinant of any $k \times k$ matrix made from columns of the combining matrix $T$ in step three of EHC.
  Larger $p$ implies larger collision probability; doubling $p$ increases $\varepsilon$ by a factor of $2^k$.
\end{description}

HalftimeHash has the following steps that will be analyzed in order:

\begin{enumerate}
\item The parts of EHC:
  \begin {enumerate}
  \item Encode
  \item Hash
  \item Combine
  \end{enumerate}
\item Tree hash
\item Sweep
  \begin{enumerate}
  \item Sweep the $k$ stacks
  \item Sweep the untouched tail of the original input string
  \end{enumerate}
\item{} {\em [optional]} Finalize with tabulation hashing or SipHash
\end{enumerate}

%%A summary of the conclusions is in Table \ref{analysis-table}.

%% \begin{table*}
%% \begin{tabular}{|l|c|c|c|c|c|c|c|}
%%   \hline {\bf Step} & {\bf Mults (each)} & {\bf In Entropy (each)} & {\bf Out Entropy (each)} & {\bf Height} & {\bf Number} & {\bf Out words total}\\
%%   \hline (1) & $b e(w-1)$ & $e(w-1)$ & $k(32-p)$ & 1 & $\lfloor n/b d w \rfloor$ & $b k \lfloor n/b d w \rfloor$ \\
%%   \hline (2) & $b (f-1)$ & $f-1$ & $32$ & $\left\lfloor\log_f\lfloor n/b d w \rfloor \right\rfloor$ & $k \lfloor n / b d w \rfloor / f$ & $b f k \left\lfloor\log_f\lfloor n/d w b \rfloor\right\rfloor$ \\
%%   \hline (3)(a) & $b f \left\lfloor\log_f\lfloor n/b d w \rfloor\right\rfloor$ & $b f \left\lfloor\log_f\lfloor n/b d w \rfloor\right\rfloor$ & $32$ & 1 & $k$ & $k$ \\
%%   \hline (3)(b) & $b d k w$ &  $b d w + k - 1$ & $32$ & 1 & 1 & $k$ \\
%%   \hline (4) & 0 &  $2^{11}k$ & $64$ & 1 & 1 & 1 \\
%%   \hline
%% \end{tabular}
%% \caption{\label{analysis-table}
%%   Statistics for number of multiplications, amount of input entropy (in 64-bit words), and output entropy (counted in bits) of each step in HalftimeHash.
%%   Additionally, each step has a height and width listed.
%%   Height is the longest dependency chain between executions of the primitive in the listed step.
%%   Number is the total number of executions of that primitive in an execution of HalftimeHash.
%%   The multiplications in total for a step is the product of the multiplications in each times the number of executions of that primitive.
%%   The collision probability is the product of the height and the collision probability of each execution of the primitive.
%% }
%% \end{table*}

\subsection{EHC}

In each EHC execution, $d w$ blocks are read in.
They are converted to $e w$ blocks deterministically, without using any input entropy.
Those $e w$ blocks are then hashed with NH (the AU version) to $e$ blocks.
This uses $e(w - 1)$ words of input entropy and performs $e(w - 1)$ multiplications.

%% -------------------------

%% Another appealing aspect of the NH primitive is that it is not only almost-universal, it is also {\em$ \varepsilon$-almost $\Delta$-universal} ($\varepsilon$-A$\Delta$U):
%% \[\forall \delta \in C, \forall x,y \in D, x \neq y \implies \mathrm{Pr}_{s \in S}[H(s, x) - H(s, y) = \delta] \leq \varepsilon \]
%% Almost-$\Delta$ universality reduces the number of multiplications required.
%% Given an almost-$\Delta$ universal family like $H$ from $C^k$ to $C$ for any $k \geq 1$, define a hash family $H'$ from $C^{k+1}$ to $C$ as $H'_s (d, x) \triangleq H_s(d) + x$, for $x \in C$; $H'$ is AU, though not A$\Delta$U.
%% This saves a multiplication over a construction like $H_s(d) + H_{s'}(x)$.
%% In HalftimeHash, the multiplication savings trade off with increased entropy requirements via a parameter the caller can set.

%% Additionally, the other components of HalftimeHash do not depend on EHC being A$\Delta$U, only AU.
%% Just as in NH, this saves HalftimeHash a percentage of multiplications.


%% -----------------------

As for collision probability, Nandi proved that in an environment in which the EHC matrix product is over a finite field, EHC is $2^{-32k}$-AU.
$\ints_{2^{64}}$ is not a finite field, but the structure of this proof that EHC in HalftimeHash is AU is similar to the structure of Nandi's proof, so a variant of that is presented here first.  \cite{ehc-nandi}

\begin{lemma}
  In EHC, when the matrix is over a field and the hash function used in step 2 is $\varepsilon$-A$\Delta$U, EHC is $\varepsilon^k$-A$\Delta$U.
\end{lemma}
\begin{proof}
  Let $H$ be the hash family used in step 2 of EHC, and let $\bar{H}$ be defined as $\bar{H}(s, x)_i = H(s_i, x_i)$.
  Let $g$ be the encoding function that acts on $x$ and $y$.
  Given that $x$ and $y$ differ, let $F$ be $k$ locations where $g(x)_i \neq g(y)_i$.
  Let $T|_F$ be the matrix formed by the columns of $T$ where the column index is in $F$ and let $\bar{H}|_F$ similarly be $H$ restricted to the indices in $F$.
  We want to bound
  \begin{equation}
    \label{ehc-delta}
    \max_\delta \Pr_s[T|_F \bar{H}|_F(s, g(x)) - T|_F \bar{H}|_F(s, g(y)) = \delta]
  \end{equation}
  where $H$ is the hash function used in step 2 of EHC.

  Since any $k$ columns of $T$ are independent, $T|_F$ is non-singular, and the equation is equivalent to $\hat{H}|_F(s, g(x)) - \hat{H}|_F(s, g(y)) = {T|_F}^{-1} \delta$, which implies
  \[
  \bigwedge_{i \in F} H(s_i, g(x)_i) - H(s_i, g(y)_i) = \beta_i
  \]
  where $\beta \triangleq {T|_F}^{-1} \delta$.

  Since the $s_i$ are all chosen independently, the probability of the conjunction is the product of the probabilities, showing
  \[
  \begin{array}{rl}
    & \max_\delta \Pr_s[T|_F \bar{H}|_F (s,g(x)) - T|_F \bar{H}|_F(s,g(y)) = \delta] \\
  \leq & \max_\beta \prod_{i \in F} Pr_s[H(s_i, g(x)_i) - H(s_i, \bar{y}_i) = \beta_i]
  \end{array}
  \]
  and since $H$ is A$\Delta$U, this probability is $\varepsilon^k$.
\end{proof}

\begin{theorem}
  The EHC step of HalftimeHash is $2^{k(p-32)}$-AU.
\end{theorem}

\begin{proof}
  In HalftimeHash, the proof of the lemma above unravels at the reliance upon the trivial kernel of $T|_F$.

  The columns of $T$ in HalftimeHash are linearly independent in $\ints$, so the matrix $T|_F$ is non-singular in $\ints^{k\times{}k}$, but not necessarily non-singular {\em in $\ints_{2^{64}}^{k\times{}k}$} because every even number in $\ints_{2^{64}}$ is a zero divisor.

  However, even in non-field rings like $\ints_{2^{64}}$, every matrix $A$ has a kind of pseudo-inverse $A^?$ such that $A \cdot A^? = A^? \cdot A = \det(A) I$.
  This pseudo-inverse is just $\det(A)$ times the inverse of $A$ in $\rats^{k\times{}k}$.
  Let $\det(T|_F) = q2^p$, where $q$ is odd.
  Now (\ref{ehc-delta}) reduces to
  \[
  \begin{array}{rl}
    &  \max_\delta \Pr_s[T|_F \bar{H}|_F(s,x) - T|_F \bar{H}|_F(s,y) = \delta]\\
    \leq & \max_\delta \Pr_s[{T|_F}^? T|_F \bar{H}|_F(s,x) - {T|_F}^? T|_F \bar{H}|_F(s,y) = {T|_F}^? \delta] \\
    = & \max_\delta \Pr_s[q2^p\bar{H}|_F(s,x) - q2^p\bar{H}|_F(s,y) = {T|_F}^? \delta] \\
    = & \max_\delta \Pr_s[2^p\bar{H}|_F(s,x) - 2^p\bar{H}|_F(s,y) = q^{-1} {T|_F}^? \delta]
  \end{array}
  \]

  Now letting $\beta = q^{-1} {T|_F}^? \delta$ and letting the modulo operator extend pointwise to vectors, we have

  \[
  \begin{array}{rl}
    = & \max_\beta \Pr_s[\bar{H}|_F(s,x) - \bar{H}|_F(s,y) \equiv \beta \bmod 2^{64-p}] \\
    = & \max_\beta \Pr_s\left[\bigwedge_{i \in F} H(s_i,x_i) - H(s_i,y_i) \equiv \beta \bmod 2^{64-p}\right] \\
    = & \max_\beta \prod_{i \in F} \Pr_s\left[ H(s_i,x_i) - H(s_i,y_i) \equiv \beta \bmod 2^{64-p}\right] \\
    = & \left(2^p 2^{-32}\right)^{|F|} = 2^{k(p-32)}
  \end{array}
  \]
  This quantity is highest when $p$ is at its maximum over all potential sets of columns $F$.

  % TODO: this can be reduced by shuffling the column order
  
\end{proof}



%%So following Table \ref{analysis-table}, we have

For this step we have the following metrics:

%% \begin{tabular}{|l|c|c|c|}
%%   \hline {\bf Step} & {\bf Mults (each)} & {\bf In Entropy (each)} & {\bf Out Entropy (each)}\\
%%   \hline EHC & $b e(w-1)$ & $e(w-1)$ & $k(32-p)$\\
%%   \hline
%% \end{tabular}
\begin{tabular}{|r|l|}
    %% \hline {\bf Step} & {\bf EHC} \\
    \hline {\bf Multiplications (each)} & $b e w$ \\
    \hline {\bf Multiplications (total)} & $b e w \lfloor n / b d w\rfloor$ \\
    \hline {\bf In Entropy} & $e w$ \\
    \hline {\bf Out Entropy} & $k (32-p)$\\
    \hline {\bf Output words (total)} & $k b \lfloor n / b d w\rfloor $\\
    \hline
\end{tabular}


\subsection{Tree hash}

For the tree hash portion of HalftimeHash, the height of the $k$ trees drives multiple metrics.
Each tree has $\lfloor n / b d w \rfloor$ blocks as input and, when a block is written to level $i$ for the first time, the execution is a complete $f$-ary tree.
The height of the tree is thus $\left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$.

\begin{lemma}
The tree hash portion of EHC produces $32k - k\lg\left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor$ bits of entropy.
\end{lemma}
\begin{proof}
  Carter and Wegman showed that tree hash has collision probability of $m \varepsilon$, where $\varepsilon$ is the collision probability of a single node in the tree and $m$ is the height of the tree.
  Each tree node uses NH, so a single tree has collision probability $\left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor 2^{-32}$.
  A collision occurs for HalftimeHash at the tree hash stage if and only if each tree has a collision, which has probability $\left(\left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor 2^{-32}\right)^k$, assuming that the EHC step didn't already induce a collision.
\end{proof}

%% The proof from Carter and Wegman does not need the primitive to be A$\Delta$U.
%% As a result, EHC doesn't need to be A$\Delta$U, which means that the NH variant used in it does not need to be A$\Delta$U.

The amount of entropy needed is proportional to the height of the tree, with $f$ words needed for every level.
Here, HalftimeHash uses different seeds for the $k$ different trees to maintain independence of the entropy in the words of the output values, so the total number of 64-bit words of entropy used in the tree hash step is $f k\left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor$.

Finally, the data in the stack is also proportional to the height and the number of trees.
Specifically, there are up to $b f$ words in each stack level, and there are $k \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$ stack levels.

In the limit, the number of multiplications performed is nearly identical to the number of input words.
Of each $m$ words fed into the tree hash phase, $7/8$ths of them are multiplied by something in the first level.
Since each level is identical (except for seeds), this continues, approaching 1 from below as the sum of a geometric series, creating a total of $k b \lfloor n / b d w \rfloor$ multiplications.

\begin{tabular}{|r|l|}
%    \hline {\bf } & {\bf Tree hash} \\
    \hline {\bf Multiplications (each node)} & $b (f-1)$ \\
    \hline {\bf Multiplications (total)} & $k b \lfloor n / b d w \rfloor$ \\
    \hline {\bf In Entropy (each tree $\times$ level)} & $f-1$ \\
    \hline {\bf In Entropy (total)} & $k (f-1) \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$ \\
    \hline {\bf Out Entropy (total)} & $32k - k\lg\left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor$\\
    \hline {\bf Output words (total)} & $b f k \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$\\
    \hline
\end{tabular}


\subsection{Sweep}

For the output words in the stack after tree hash, HalftimeHash uses NH, which uses $b f k \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$ words of entropy and just as many multiplications.

There can also be as much as $b d w$ words of data in the raw input that are untouched so far by HalftimeHash.
Again, NH is used, but now hashing $k$ times, since this data has not gone through EHC.
That requires $b d w k$ words of entropy and just as many multiplications.

For the untouched data, the number of words of entropy needed can be reduced by nearly a factor of $k$ using the Toeplitz construction.
Let $r$ be the sequence of random words used in the leftover part of the sweep.
Instead of using $[r_{i b d w}, r_{(i+1)b d w})$ as the keys to hash component $i$ with, HalftimeHash uses $[r_{i}, r_{b d w + i})$.
Both Nandi and Woelfel present short proofs that this construction for multi-part hash output is A$\Delta$U. \cite{ehc-nandi,woelfel-toeplitz}

%Instead of computing the $i$th component of the output using $b d w$ new words of entropy, the ith component 

\subsection{Cumulative analysis}

The combined collision probability is
\[2^{k(p-32)} + \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor^k 2^{-32k} + 2^{-32k}\]
For the main example described above, with $k=3$, $p=2$, $f=8$, $d = 7$, and $w=3$, and for strings less than an exabyte in length, this is more than 83 bits of entropy.

The combined input entropy needed (in words) is

\[
e w
+ (f-1) k \left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor
+ b f k \left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor
+ b d w + k - 1
\]

When $b$ is 8, as in the AVX-512 version of HalftimeHash, this is less than 31KB for strings of length up to one exabyte -- 8.4KB for strings of length up to one megabyte.
6.1KB of this is for hashing the stack.
This could be reduced by hashing the stacks with unmodified tree hash and hashing the remaining untouched input string with another instantiation of HalftimeHash with smaller block size, but this adds complexity to the algorithm and reduces performance.

The number of multiplications is asymptotically driven by the EHC step, since the total is $(e w + k) b \lfloor n / b d w \rfloor + O(\log n)$ and $e w$ is significantly larger than $k$.

For a string of length 1MB, 84\% of the multiplications happen in the EHC step. %, and the number of multiplications is about one per ten bytes of input.
Intel's VTune tool show the same thing: 86\% of the clock cycles are spend in the EHC step.

%% \begin{tabular}{|l|c|c|c|c|c|c|}
%%   \hline {\bf Step} & {\bf Multiplications} & {\bf In Entropy} & {\bf Out Entropy} & {\bf Out words}\\
%%   \hline EHC & $e(w-1) \$ & $(32-p)k$ & 1 & $\lfloor n/dw \rfloor$ & $k \lfloor n/dw \rfloor$ \\
%%   \hline Tree Hash & $f-1$ & $32k$ & $\left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ & $(f-1) k \lfloor n / d w \rfloor$ & $f k \left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ \\
%%   \hline Sweep stack & $b f k \left\lfloor\log_f\lfloor n/d w \rfloor\right\rfloor$ & $32k$ & 1 & 1 & $k$ \\
%%   \hline Sweep input & $n / b d w$ & $32k$ & 1 & 1 & $k$ \\
%%   \hline
%% \end{tabular}



%% Finally, for the tabulation hashing of 32 bytes, \texttt{32 * 256 * 8 = 64KiB} of entropy is required.
%% This is the majority of all of the entropy used by HalftimeHash.
%% In entropy-constrained environments, users may wish to substitute finalizers that use less entropy, such as SipHash. \cite{siphash}

%% Overall the cleaup requires $8 f \lfloor \log_f (n/3B) - 1 \rfloor + 66 B + 65536$ bytes of entropy.

%% As for entropy provided, partially because of the long keys used in this section, the output before tabulation hashing is $(2^{-90} + 2^{-86} + 2^{-96})$-almost universal, with the $90$ coming from EHC and the $86$ coming from treehash, for a total entropy of over $85.9$ bits of entropy.
%% Tabulation adds $2^{-64}$, for a total of over $63.99$ bits of entropy.

%% After EHC, HalftimeHash essentially runs three treehashes.
%% Two input strings collide if they collide on all three treehashes.
%% Because the keys for each treehash were chosen indepenently, there is no dependence between a collision in one of the three parts and a collision in either of the other parts.

%% The collision probability of treehash is proportional to the product of the log of the input length and the collision probability of the basic hash function, which for HalftimeHash is NH.

%% If $w \neq x$, then $\Pr_s[H_i(s, w) = H_i(s, x)]$ is no more than $\varepsilon$.
%% This can be applied to each block without rasing $\varepsilon$:
%% If $[w, x] \neq [y, z]$, then either $w \neq y$, $x \neq z$, or both.
%% W.l.o.g, assume $w \neq y$; now if $G_i(s, [w, x]) = G_i(s, [y, z])$, then $H_i(s, w) = H_i(s, y)$, so \[Pr_s[G_i(s, [w, x]) = G_i(s, [y, z])] \leq \Pr_s[H_i(s, w) = H_i(s, y)] \leq \varepsilon\] and thus $G_i$ is $\varepsilon$-almost universal.
%% The extra appended characters do not alter this - the identity function is $0$-almost universal!

%% These $G_i$ can be applied repeatedly, starting with $G_1$ until the string remaining has fewer than $m$ characters.
%% At each level, $\varepsilon$ grows.
%% It is rather direct, and Carter \& Wegman show, that for any $U$ that is $\varepsilon_1$-almost universal and $V$ that is $\varepsilon_2$-almost universal, their composition $U \circ V$ is $(\varepsilon_1 + \varepsilon_2)$-almost universal.

%% The number of times $G_i$ needs to be applied is $\log_{m/j}(n/m) \pm O(1)$, so $\widehat{G} \triangleq  \bigcirc_i G_i$, which takes a seed from $\prod_i S_i$ and an input string of length $m$, is $\widehat{\varepsilon}$-almost universal, where $\widehat{\varepsilon} \triangleq (\log_{m/j}(n/m) \pm O(1)) \prod_i \varepsilon_i$.

%% This only applies to strings of length exactly $m$, but taking the output of $\widehat{G}$ and appending the value $m$ is a family that is $\widehat{\varepsilon}$-almost universal on strings of arbitrary length.

%% As shown in \cite{carter-wegman-79}, composing hash families like this incurs a linear extra cost: the composition of an $\varepsilon_1$-almost universal hash function and an $\varepsilon_2$-almost universal hash function is $(\varepsilon_1 + \varepsilon_2 - \varepsilon_1 \varepsilon_2)$-almost universal.
%% The number of functions composed depends on the height of the stack, which depends on the length of the string.

%% Each level in the stack/tree reduces the height of the string by a factor of \texttt{kFanout}.
%% Since a level needs \texttt{kFanout * 3 Blocks} before populating the next level, the total number of levels is
%% \[\lfloor \log_f n - \log_f (3fB) \rfloor = \lfloor \log_f (n/3B) - 1 \rfloor \]
%% where $f$ is \texttt{kFanout} and $B$ is \texttt{sizeof(Block)}.
%% For a string of 1TiB, $f = 8$, and $B = 64$, this is $10$.
%% Since the tree hash is essentially performing three almost-universal hashings with 32 bits of entropy per level, the total end entropy is $\lg (2^{32} / 10)^3 > 86$ bits.


%% The input seed must have enough entropy to hash all of the encoded values down from a three-tuple to a single value.
%% HalftimeHash hashes these almost-$\Delta$-universally using NH (see below).
%% This requires one 64-bit word for each input word, which is $9 \cdot 3 = 27$ based on $e$ and $i_w$.


%% Since the sums differ in at least one lcoation, this probability is at least

%% This is at least

%% \texttt{EhcUpperLayer} takes 1536 bytes as input and produces 256 bytes as output.
%% Physically these are each contiguous bytes in memory, but {\em logically}, the input should be thought of three {\em partitions}, each consisting \texttt{sizeof(Block) / sizeof(uint64\_t) = 8} interleaved {\em instances} of NH; each instance is broken up into \texttt{fanout} 64-bit {\em words}.
%% To hash an instance, \texttt{fanout - 1} words are hashed with NH and then added to the last word in the instance.
%% Thus each instance requires \texttt{fanout - 1} words of entropy, but they can all use the same entropy, since each can be thought of as a group of leaves in the treehash, with no overlapping groups.

%% Each instance has 32 bits of entropy and is essentially a portion of all the treehash edges at a given level.
%% As a whole, this family produces 32 bits of entropy.
%% However, the partitions will be essential to the bounds discussed below, so we need to dive deeper here.

%% We will see below that if the input to two different calls to \texttt{EhcUpperLayer} differ in one bit of input, they differ in all three partitions with high probability.
%% As such, the output of \texttt{EhcUpperLayer} differs for different input with probabilty $1 - ({2^{-32}})^3$.

\section{Tests}

This section covers performance testing for HalftimeHash compared to clhash and UMASH, two AU families that can hash long strings.
Each of those are based on carryless (or ``polynomial'') multiplication over $\ints_2$ and a version of NH.

HalftimeHash can vary over three dimensions:

\begin{itemize}
\item Block size, the number of instances interleaved in each execution
\item Output size, which varies from 16 bytes to 40 bytes in this implementation
\item ISA, which varies from the C++ virtual machine (in other words, scalar operations on \texttt{uint32\_t}s) to AVX-512
\end{itemize}

A small sample of these are presented here, focused on large block sizes and ISA's with at least AVX2.

In addition to the performance testing here, HalftimeHash passes all correctness tests in the SMHasher test suite.\cite{smhasher}

\subsection{Throughput vs. output entropy}

Figure \ref{frontier} displays the tension between higher output entropy and higher throughput for 250KB strings.
Adding more output entropy increases the number of multiplications and additions that HalftimeHash has to perform.
Nandi showed that this is true in the general case, as there is a matching upper and lower bound for the number of non-linear operations to be performed for a certain hash output width.

Three frontiers emerge, in which HalftimeHash always has better performance and lower collision probability than clhash, and clhash similarly dominates UMASH.

\includegraphics[width=8cm]{speed-v-epsilon.eps}
\captionof{figure}{
  \label{frontier}
  Trade-offs for almost-universal string hashing functions on strings of size 250KB on an i7-7800x with AVX-512.
  The upper right corner is ideal: low execution time and low collision probability.
  Note that the clhash and UMASH executions are not along the efficient frontier / layer-of-maxima, in that for each clhash / UMASH version, at least one version of HalftimeHash is faster and has lower collision probability. \protect\cite{layer-of-maxima}
  This contrasts with the case of short strings, where clhash and UMASH have better performance.
}

%% The configuration matrix consists of four block sized, from 64 to 512 bits, 

%% Another variant of HalftimeHash varies based on the block width.
%% Variants labelled ``v4'' use 512-bit block, v3, 256 bit blocks, and so on.
%% The code currently sixteen total variants, with four output widths and four block widths.

%% TODO: Each of the variants of HalftimeHash for different SIMD widths and output size pass all SMhasher tests when finalized with tabulation hashing. \cite{smhasher}
%% Each also passes the clhash tests.
%% Additionally, the AVX-512 variants of HalftimeHash16 and HalftimeHash24 exceed all other hash functions in SMhasher in terms of speed.

Figure \ref{output-variants} shows the variation of HalftimeHash throughput between output variants across input sizes.

\includegraphics[width=8cm]{clang-local-hh4.eps}
\captionof{figure}{
  \label{output-variants}
  Different output widths of HalftimeHash.
  %Note that even the family with the longest output of 40 bytes is compet than clhash and UMASH; see Figure \protect\ref{frontier}.
  The sharp drops in performance correspond to the L2 and L3 data cache sizes.
}

Figure \ref{vs-cl} adds comparisons between clhash, UMASH, and HalftimeHash across input sizes and ISA's.
Although these machines support different ISA vector extensions, the pattern is similar: at high enough volume, HalftimeHash's throughput exceeds that of the carryless-multiplication families.
The ``v3'' after the name indicates block size: v3 means a 256-bit block size, while v4 (the default) means 512-bit block size.
Figure \ref{block-size} shows how the smaller block size performs better on the AMD chip.
%Cross-machine workloads or persistent hash values may require a consistent block size between machines with different maximum SIMD register sizes, but Figure \ref{block-size} shows the performance penalty is not unreasonable.

\begin{figure*}
\begin{tabular}{cccc}
\includegraphics[width=8.5cm]{line-cl-hh24.eps}
&
\includegraphics[width=8.5cm]{amd-cl-hh24.eps}
\end{tabular}
\caption{
  \label{vs-cl}
  Comparison of Intel (i7-7800x) and AMD (EC2 c5a.large, 7R32 chip) performance.
  These AMD chips do not support AVX-512, but still HalftimeHash with 256-bit registers exceeds the speed of clmul-based hashing methods by up to a factor of 2 on long strings.
  In both cases, for long strings, HalftimeHash with 24 bytes of output is faster than clhash and UMASH.
  HalftimeHash24 also has lower collision probability.
  For long strings, the incremental cost of hashing the 24 bytes down to 8 with tabulation hashing is insignificant.
}
\end{figure*}

\begin{figure*}
\begin{tabular}{cccc}
\includegraphics[width=8.5cm]{amd-16.eps}
&
\includegraphics[width=8.5cm]{amd-24.eps}
\end{tabular}
\caption{
  \label{block-size}
  AMD performance.
  AMD chips do not have AVX-512 instructions, so blocks of that size are simulated with 2 AVX2 registers.
  Using a block size of 256 bits performs better.}
\end{figure*}

%% TODO: compare to VHASH

%% HalftimeHash24, when composed with SipHash or tabulation hashing as a final step, passes all SMHasher tests.
%% This is faster in SMhasher's bulk speed tests than all other previously tested hash function, excluding \texttt{crc32\_pclmul\_le\_16}, a deterministic 32-bit hash function implemented in assembly in the Linux kernel.

%% TODO: Notes on SMHasher and relatives

%% TODO: crypto community tests, eBASH

\section{Future work}

HalftimeHash is a fast family for strings that are more than 1KB in size.
Above was pondered combining HalftimeHash with hash families designed for shorter strings.
The details of that combination remain future work.

Additionally, HalftimeHash depends on platforms supporting multiplication of two 32-bit integers that produces a 64-bit integer.
This operation is present in many environments, but not all.
Notable exceptions include some embedded platforms that do not support widening multiplication of two 32-bit integers and native JavaScript, which only supports 53-bit integers.

More detailed comparisons against hash algorithms written for speed and executed in the Linux kernel, including Poly1305 and \texttt{crc32\_pclmul\_le\_16} could also be a fruitful direction for future work.

%TODO: multi-threading

%TODO: reducing seed size

%\lstinputlisting{narrow-code/halftime-hash-basic.c}

\begin{acks}
Thanks to Daniel Lemire and Paul Khuong for helpful discussions.
\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{library}

%% \appendix
%% \section{Additional utilities}

%% An additional block type is a repetition of multiple blocks of smaller size.
%% This has utility, when, for instance, the same hash function must be used on different machines with different available SIMD instructions.


%% The remainder describes the user-facing functions, all of which wrap the \texttt{Hash()} function above.

%% %

\end{document}
\endinput
%% The last equation is equivalent to $H(s,x) \equiv H(s,y) 2^{64 - p}$, with operations in $\ints^k$, rather than $\ints_{2^{64}}^k$.
%% This is equivalent to $\exists 0 \leq r < 2^p, 2^p(H(s,x) - H(s,y)) = \delta' + 2^{64} r$.
%% Since these operations are pointwise on three components, there are not just $2^p$ values, but $(2^p)^3$ values of $r$, so by the union bound we have
%% \[
%% \max_{\delta' \in \Phi^3}, \Pr_s[2^p(H(s,x) - H(s,y)) = \delta'] \leq (2^r\varepsilon)^3
%% \]
%% Since $r=2$ and $\varepsilon = 2^{-32}$, this is $2^{-90}$.


%% and has an inverse $W^{-1}$, so this equation is equivalent to $H(s,x) - H(s,y) = W^{-1} \cdot \delta$.
%% \begin{equation}
%%   \begin{array}{rl}
%%   \label{ehc-delta-final}
%%   &\max_{\delta \in \Phi^3}, \Pr_s[H(s,x) - H(s,y) = W^{-1} \cdot \delta] \\
%%   \leq&   \max_{\delta' \in \Phi^3}, \Pr_s[H(s,x) - H(s,y) = \delta']
%% \end{array}
%% \end{equation}

%% By \ref{ehc-h-delta}, this is $\leq \varepsilon^3$.


%% Note that this bound is not tight.
%% In particular, when $\delta'$ is not dividible by $2^p$, $\max_{\delta' \in \Phi^3}, \Pr_s[2^p(H(s,x) - H(s,y)) = \delta'] = 0$


%% Let $H$ be the hash function in step 1 of EHC and $G$ be the full EHC hash family, including all three steps.
%% The output size of EHC is three 64-bit words with the property that if $x \neq y$ are input strings and $\delta$ is a three-tuple in $\ints_{2^{64}}^3$, then
%% \[
%% \forall_{i < 3}, \Pr_s[G(s,x)_i = G(s,y)_i + \delta_i] = 2^{-32}
%% \]
%% and
%% \[
%% \Pr_s\left[\forall_{i < 3}, G(s,x)_i = G(s,y)_i + \delta_i\right] = 2^{-96}
%% \]
%% In other words, $G$ is $2^{-32}$-almost-$\Delta$ universal and the $3$ output values of $G$ are independent.

%% First, the proof of independence:

%% Nandi's proof of this relies on the invertability of non-singular matrices to preserve entropy:
%% Let $w, z$ be two unequal input sequences from $\Sigma^m$.
%% The encoding produces $x, y$ of length $m + 2$ with minimum distance three.
%% That is, $\exists i < j < k \leq m + 2, x[i] \neq y[i] \wedge x[j] \neq y[j] \wedge x[k] \neq y[k]$.

%% The hash functions are picked independently from an $\varepsilon$-almost-$\Delta$ universal family $H \in S \to \Sigma \to \Phi$, so
%% \begin{equation}
%%   \label{ehc-h-delta}
%%   \forall \delta \in \Phi^3, \Pr_s[H(s,x) = H(s,y) + \delta] \leq \varepsilon^3
%% \end{equation}
%% Individually, each component the three components in the equation is true with probability $\varepsilon$, but we need the fact that if $r \neq t$ then for all $z$, $H(s_r, z_r)$ and $H(s_t, z_t)$ are independent to get the $\varepsilon^3$ bound.


%% That's how much entropy is provided.
%% The entropy required is three 64-bit keys per column, since we collapse the three-part input into one part before applying the combination transform.
%% That's a total of \texttt{3 * 9 * sizeof(uint64\_t) = 216} bytes.

%% \[
%% \forall_{i < 3}, \Pr_s[G(s,x)_i = G(s,y)_i + \delta_i] = 2^{-32}
%% \]

%% Since $x$ and $y$ differ, their encodings differ in at least three places.
%% The matrix used in the Combine step has no more than two zeros in a row.\footnote{In fact, any matrix in which all sets of three columns are non-singular have this property.}
%% This means there is at least one non-zero column in row $i$ where $x$ and $y$ differ before the hashing step.
%% After the NH hashing step, each location still differs with probability $1-2^{-32}$.


%% % TODO: does this work better if at most one zero in each column?

%% \[
%% \Pr_s \left[\sum_j V_{i,j}  H(s_j, e(x)) = \sum_j V_{i,j}  H(s_j, e(y)) + \delta\right]
%% \]

%% Wlog, let $m$ be a non-zero location in $V_i$ where $e(x)$ and $e(y)$ differ.
%% The above probability now can be converted to

%% \[
%% \Pr_s \left[V_{i,m} H(s_m, e(x)) =  V_{i,m} H(s_m, e(y)) + \delta'\right]
%% \]

%% Where $\delta' = \sum_{j\neq m} V_{i,j}  (H(s_j, e(y)) - H(s_j, e(x))) + \delta$.
%% Let $V_{i,m} = p2^q$, where $p$ is odd.
%% The probability above is now equivalent to

%% \[
%% \Pr_s \left[2^q H(s_m, e(x)) =  2^q H(s_m, e(y)) + \delta''\right]
%% \]

%% where $\delta'' = q^{-1}\delta'$.
%% This is possible because $q$ has an inverse in $\ints_{2^{64}}$.
%% This is identical to the analysis above regarding even numbers and inverses, and the probability is no more than $2^{q-32}$.
%% Since $q$ is at most 2 in the V we use

%% By the almost-$\Delta$ universality of $H$, $\Pr_s \left[V_{i,m} H(s_m, e(x)) =  V_{i,m} H(s_m, e(y)) + \delta'\right]$

% passes smhasher: poly_2_mersenne poly_3_mersenne poly_4_mersenne
% tabulation floppsyhash GoodOAAT prvhash42_128 prvhash42_64
% prvhash42s_64 prvhash42s_128 HighwayHash64 mirhashstrict pengyhash
% City64low City128 CityCrc128 FarmHash64 FarmHash64 FarmHash128
% xxHash64 xxh3low xxh128 xxh128low wyhash umash64

%%  LocalWords:  HalftimeHash codomain ISA's UMASH falkhash MeowHash
%%  LocalWords:  MetroHash FarmHash clhash wyhash farmhash UHASH EHC
%%  LocalWords:  VHASH Nandi's ISA strided Wegman TODO VTune NH's XXH
%%  LocalWords:  UMAC VMAC carryless fanout Nandi Gabrielyan Toeplitz
%%  LocalWords:  Gabrielyan's HalftimeHash's Woelfel SMHasher PVLDB
%%  LocalWords:  VLDB zeroless Skylake uint clmul pclmul Lemire
%%  LocalWords:  Khuong
