\documentclass[runningheads]{llncs}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage[pdfusetitle]{hyperref}
\usepackage{microtype}

\DeclareMathOperator{\adj}{adj}

\renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\rats}{\mathbb{Q}}
\newcommand{\nats}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\cplx}{\mathbb{C}}
\newcommand{\defeq}{\;\genfrac{}{}{0pt}{2}{\text{def}}{=}\;}

\newenvironment{blockquote}
{\begin{quote}\itshape}
{\end{quote}}


\begin{document}

\title{HalftimeHash: modern hashing without 64-bit multipliers or finite fields}
\author{Jim Apple
\orcidID{0000-0002-8685-9451}}
\institute{\email{jbapple@apache.org}}

\maketitle

\begin{abstract}
HalftimeHash is a new algorithm for hashing long strings.
The goals are few collisions (different inputs that produce identical output hash values) and high performance.

Compared to the fastest universal hash functions on long strings (clhash and UMASH) HalftimeHash decreases collision probability while also increasing performance by over 50\%, exceeding 16 bytes per cycle.

In addition, HalftimeHash does not use any widening 64-bit multiplications or any finite field arithmetic that could limit its portability.

\keywords{Universal hashing \and Randomized algorithms}
\end{abstract}

\section{Introduction}
A hash family is a map from a set of seeds $S$ and a domain $D$ to a codomain $C$.
A hash family $H$ is called is $\varepsilon$-almost universal (``$\varepsilon$-AU'' or just ``AU'') when
\[
\forall x,y \in D, x \neq y \implies \mathrm{Pr}_{s \in S}[H(s, x) = H(s, y)] \leq \varepsilon \in o(1)
\]
The intuition behind this definition is that collisions can be made unlikely by picking randomly from a hash {\em family} independent of the input strings, rather than anchoring on a specific hash {\em function} such as MD5 that does not take a seed as an input. AU hash families are useful in hash tables, where collisions slow down operations and, in extreme cases, can turn linear algorithms into quadratic ones. \cite{tabulation,rust-quadratic,impala-quadratic,algorithm-attack}

HalftimeHash is new a ``universe collapsing'' hash family, designed to hash long strings into short ones. \cite{linear-hash-functions,hashing-without-primes-revisited,cuckoo-journal}
This differs from short-input families like SipHash or tabulation hashing, which are suitable for hashing short strings to a codomain of 64 bits. \cite{siphash,tabulation}
Universe collapsing families are especially useful for composition with short-input families: when $n$ long strings are to be handled by a hash-based algorithm, a universe-collapsing family that reduces them to hash values of length $c \lg n$ bits for some suitable $c > 2$ produces zero collisions with probability $1-O(n^{2-c})$.
A short-input hash family can then treat the hashed values as if they were the original input values. \cite{universe-collapse-linear-probing,siphash,tabulation,simple-hash-functions-work}
This technique applies not only to hash tables, but also to message-authentication codes, load balancing in distributed systems, privacy amplification, randomized geometric algorithms, Bloom filters, and randomness extractors. \cite{poly1305,chord,privacy-amplification,random-closest-pair,simple-hash-functions-work,fuzzy-extractors}

On strings longer than 1KB, HalftimeHash is typically 55\% faster than clhash, the AU hash family that comes closest in performance.

HalftimeHash also has tunable output length and low probabilities of collision for applications that require them, such as one-time authentication. \cite{nacl}
The codomain has size 16, 24, 32, or 40 bytes, and $\varepsilon$ varies depending on the codomain (see Figure \ref{frontier} and Section \ref{analysis}).

\subsection{Portability}

In addition to high speed on long strings, HalftimeHash is designed for a simple implementation that is easily portable between programming languages and machine ISA's.
HalftimeHash uses less than 1200 lines of code in C++ and can take advantage of vector ISA extensions, including AVX-512, AVX2, SSE, and NEON.

Additionally, no multiplications from $\ints_{2^{64}} \times \ints_{2^{64}}$ to $\ints_{2^{128}}$ are needed.
This is in support of two portability goals -- the first is portability to platforms or programming languages without native widening unsigned 64-bit multiplications.
Languages like Java, Python, and Swift can do these long multiplications, but not without calling out to C or slipping into arbitrary-precision-integer code.

% TODO: check this

The other reason HalftimeHash avoids 64-bit multiplications is portability to SIMD ISA extensions.
The x86-64 ISA extensions SSE2, AVX2, and AVX-512F all contain instructions to simultaneously multiply multiple pairs of 32-bit words, producing multiple 64-bit values.
Aarch64 has similar instructions in the NEON set.
% The POWER ISA also contains this, but only on 128 bits at once?


\subsection{Prior work}

There are a number of fast hash algorithms that run at rates exceeding 8 bytes per cycle on modern x86-64 processors, including Fast Positive Hash, falk\-hash, xxh, Meow\-Hash, and UMASH, and cl\-hash. \cite{smhasher}
Of these, only cl\-hash and U\-MASH include claims of being AU; each of these uses finite fields and the x86-64 instruction for carryless (polynomial) multiplication.
Other previous work on AU hashing long strings is no longer as competitive, performance-wise, as it once was, including UHASH and VHASH. \cite{clhash,umac,vmac}

\section{Notations and Conventions}

Input string length $n$ is measured in 64-bit words.
``32-bit multiplication'' means multiplying two unsigned 32-bit words and producing a single 64-bit word.
``64-bit multiplication'' similarly refers to the operation producing a 128-bit product.
All machine integers are unsigned.
Subscripts indicate a numbered component of a sequence, starting at 0.

$\varepsilon$ is called the {\em collision probability} of $H$; it is inversely related to $H$'s {\em output entropy}, $-\lg \varepsilon$.
The seed is sometimes referred as {\em input entropy}, which is distinguished from the output entropy both because it is an explicit part of the input and because it is measured in words or bytes, not bits.

Each step of HalftimeHash applies various transforms to groups of input values.
These groups are called {\em instances}.
The processing of a transform on a single instance is called an {\em execution}.

Instances are logically contiguous but physically strided, for the purpose of simplifying SIMD processing.
A physically contiguous set between two items in a single instance is called a {\em block}; the number of 64-bit words in a block is called the {\em block size}.
Because instances are logically contiguous, when possible, the analysis will elide references to the block size.

HalftimeHash produces output that is collision resistant among strings of the same length.
Adding collision resistance between strings of {\em different} lengths to such a hash family requires only appending the length at the end of the output.
This turns, for instance, a hash family that produces 24 bytes of output into a hash family that produces 32 bytes of output.

The main portion of the text describes a particular instantiation of HalftimeHash that produces 24 bytes of output.
This will be generalized in Section \ref{analysis}.
Variants will be specified by their number of output bytes: HalftimeHash16, HalftimeHash24, HalftimeHash32, or HalftimeHash40.
HalftimeHash24 has $\varepsilon < 2^{-83}$, as discussed in Section~\ref{analysis}.

Except where otherwise mentioned, all benchmarks were run on an Intel i7-7800x (a Skylake chip that supports AVX512), running Ubuntu 18.04, with clang++ 11.0.1.


\section{Algorithm}
\label{algo}

\subsection{Overview}

HalftimeHash is similar to a tree-like hash as described by Carter and Wegman. \cite[Section 3]{carter-wegman-79}
The leaves of a tree are the words of the input string, while the root is the output hash value.

To hash a string, we use $\lceil \lg n \rceil$ hash families $H_i$ that hash two characters down to one and the $\lceil \lg n \rceil$ randomly-selected keys $k_i$.
Now for any $n \in \nats$ let $\lfloor\!\lfloor n \rfloor\!\rfloor$ denote the largest power of $2$ that is less than $n$.
Let $\langle a,b,c \rangle$ denote a string with the characters $a$, $b$, and $c$, in that order.
Then the tree hash $T$ of a string $s[0,n)$ is defined as

\begin{equation}
\label{algebraic-badger}
\begin{array}{l}
T(k, \langle x \rangle) \defeq x \\
T(k, s[0,n)) \defeq H_{ \lg \lfloor\!\lfloor n \rfloor\!\rfloor}
(k_{ \lg \lfloor\!\lfloor n \rfloor\!\rfloor},
T(k, s[0,\lfloor\!\lfloor n \rfloor\!\rfloor),
T(k, s[\lfloor\!\lfloor n \rfloor\!\rfloor, n) )))
\end{array}
\end{equation}

Carter and Wegman show that if each $H$ is $\varepsilon$-AU, $T$ is $m\varepsilon$-AU for input that has length exactly $2^m$.
Later this was extended to strings with lengths other than are not a power of two.~\cite{badger}
The key lemma is that almost universality is composable:

\begin{lemma}[Carter and Wegman]
  If $F, G,$ and $H$ are all universal families, then so are
  \begin{itemize}
    \item $\langle F, G\rangle$ where $\langle F, G \rangle(\langle k_F, k_G \rangle, \langle x, y \rangle) \defeq \langle F(k_F,x), G(k_G,y) \rangle$.
  \item $F \circ G$ where $(F \circ G) (\langle k_F, k_G \rangle, x) \defeq F(k_F,G(k_G, x))$
  \item $F \circ \langle G,H \rangle$ where $(F \circ \langle G,H \rangle) (\langle k_F, k_G, k_H \rangle , \langle p,q \rangle) \defeq F(k_F, G(k_G,p), H(k_H, q))$ is a family parameterized by the combination of keys for $F$ , $G$, and $H$, even if $G = H$ and $k_G = k_H$.
  \end{itemize}
\end{lemma}

For HalftimeHash, define $\widehat{H}$ as a family taking as input strings of any length $n$ and producing strings of length $\lceil \lg n \rceil$ as follows:

\begin{equation}
%\label{algebraic-badger}
\begin{array}{l}
\widehat{H}_0(k, \langle \rangle) \defeq \langle \bot \rangle \\
\widehat{H}_0(k, \langle x \rangle) \defeq \langle x \rangle \\
\widehat{H}_{i+1}(k, s[0,n)) \defeq \left\{ 
  \begin{array}{rcll}
   \bot &\triangleleft& \widehat{H}_i(k, s[0,n)) &  2^i > n \\
    T(k, s[0, 2^i))) &\triangleleft& \widehat{H}_{i}(k,s[2^i, n)) & 2^i \le n
  \end{array}
  \right.
\end{array}
\end{equation}

where $\bot$ is a symbol not otherwise in the alphabet, $\triangleleft$ prepends a character onto a string.
By an induction usingg the composition lemma, $\widehat{H}$ is $\varepsilon \lg n$-AU, and $G \circ \widehat{H}$ is $(\delta + \varepsilon \lg n)$-AU if $G$ is $\delta$-AU.

This example uses families $H_i$ that take two characters as input, but this can be easily extended to hash functions taking more input (but requiring larger keys).

A novel implementation choice in HalftimeHash is using an erasure coding of the input in the leaves.
The central idea is called ``EHC'', for {\em Encode, Hash, Combine}. \cite{ehc-nandi}
The application of the erasure code adds ``minimum distance'' $k$, so that any two input values that differ in any location produce encoded outputs that differ in at least $k > 1$ locations.
This decreases the probability of collisions by a factor of $\varepsilon^{k-1}$.

The remainder of this section will introduce a number of components previously introduced in the literature, including NH, EHC, and tree hashing.
This paper synthesizes them into a single implementation and introduces an enhancement to EHC that eliminates the multiplications associated with it and replaces them with a small number of shifts and additions.

\subsection{The NH hashing primitive}

The hashing primitive used in HalftimeHash to hash small, fixed-length sequences is called {\em NH}: \cite{umac}
\[
\sum_{i=0}^m (d_{2i} + s_{2i})(d_{2i+1} + s_{2i+1})
\]
where $d, s \in \ints_{2^{32}}^{2m+2}$ are the input string and the seed, respectively.
The $d_j + s_j$ additions are in the ring $\ints_{2^{32}}$, while all other operations are in the ring $\ints_{2^{64}}$.
NH is $2^{-32}$-AU.

NH is used at the leaves of tree hash to decrease the input length of the input string by a constant factor.


\subsection{Encode-Hash-Combine}

At the leaves of the tree, HalftimeHash uses the EHC algorithm: \cite{ehc-nandi}

\begin{enumerate}
\item A sequence of items is processed by an erasure code with minimum distance $k$, producing a slightly longer encoded sequence.
\item Each item in the encoded sequence is hashed using NH with an independently chosen seed.
\item A linear transformation $T$ is applied to the resulting sequence of hash values.
  $T$ must have the property that any $k$ columns of it are linearly independent.
\end{enumerate}

This reduces the input length of the string by a constant factor.


HalftimeHash uses a non-linear erasure code discovered by Gab\-ri\-el\-yan. \cite{9-7-erasure-code}
It maps seven 3-tuples to nine 3-tuples with a minimum distance of three.


\subsubsection{Transform cost}

At first glance, EHC might not look like it will reduce the number of multiplications needed, as the application of linear transformations usually requires multiplication.
However, since $T$ is not part of the randomness of the hash family, it can be designed to contain only values that are trivial to multiply by, such as powers of $2$.

The constraint in \cite{ehc-nandi} requires that any 3 columns of $T$ form an invertible matrix.
This is not feasible in $\ints_{2^{64}}^{3 \times 9}$, as any such matrix will have at least one set of three columns with an even determinant, and which therefore has a non-trivial kernel.

\begin{proof}
  Let $U$ be a matrix over $\ints_2$ formed by reducing each entry of $T$ modulo $2$.
  Then $(\det T) \bmod 2 \equiv \det U$.
  Since there are only 7 unique non-zero columns of height 3 over $\ints_2$, by the pigeonhole principle, some two columns $x, y$ of $U$ must be equal.
  Any set of columns that includes both $x$ and $y$ has a determinant of $0 \bmod 2$.
  \qed
\end{proof}

Instead, HalftimeHash uses a matrix $T$ selected so that the largest power of 2 that divides any determinant is $2^2$.
The consequences on collision probability are discussed in Section \ref{analysis}.

HalftimeHash uses the $3 \times 9$ matrix below.

\begin{displaymath}
  \left(
\begin{array}{rrrrrrrrr}
  0 & 0 & 1 & 4 & 1 & 1 & 2 & 2 & 1\\
  1 & 1 & 0 & 0 & 1 & 4 & 1 & 2 & 2\\
  1 & 4 & 1 & 1 & 0 & 0 & 2 & 1 & 2
\end{array}
\right)
\end{displaymath}

For other output widths, HalftimeHash uses

\[
\begin{tabular}{|r|c|c|c|}
  \hline  & HalftimeHash16 & HalftimeHash32 & HalftimeHash40 \\
  \hline $T$ & 
$\left(
\begin{array}{rrrrrrrrrrrr}
  1 & 0 & 1 & 1 & 2 & 1 & 4\\
  0 & 1 & 1 & 2 & 1 & 4 & 1
\end{array}
\right)$
&
$\left(
\begin{array}{rrrrrrrrrr}
 0 & 0 & 0 & 1 & 1 & 4 & 2 & 4 & 1 & 1 \\
 0 & 1 & 2 & 0 & 0 & 1 & 1 & 2 & 4 & 1 \\
 2 & 0 & 1 & 0 & 4 & 0 & 1 & 1 & 1 & 1 \\
 1 & 1 & 0 & 1 & 0 & 0 & 4 & 1 & 2 & 8
\end{array}
\right)$
&
$\left(
\begin{array}{rrrrrrrrr}
 1 & 0 & 0 & 0 & 0 & 1 & 1 & 2 & 4\\
 0 & 1 & 0 & 0 & 0 & 1 & 2 & 1 & 7\\
 0 & 0 & 1 & 0 & 0 & 1 & 3 & 8 & 5\\
 0 & 0 & 0 & 1 & 0 & 1 & 4 & 9 & 8\\
 0 & 0 & 0 & 0 & 1 & 1 & 5 & 3 & 9
\end{array}
\right)$ \\
\hline $p$ & $2^2$ & $2^3$ & $2^3$ \\
\hline
\end{tabular}
\]

%Furthermore, the input to each section is independent? Partially independent, based on the loss at the matrix multiplication step? Since each partition

\subsection{Tree hashing}

As presented, NH and EHC only hash fixed-size blocks.
Carter and Wegman outline a simple tree-like construction of hashing to handle strings of arbitrary length. \cite{badger,carter-wegman-79}
The idea is direct composition of hash functions that take more bytes as input than they produce as output.
A more detailed description is in the overview of Section \ref{algo}.

For the tree hashing at internal nodes (above the leaf nodes, which use EHC), three tree hashes are executed with independently-chosen seeds, producing output entropy of $-3 \lg \varepsilon$.
From the result from Carter and Wegman on the entropy of tree hash of a tree of height $m$, the resulting hash function is $m\varepsilon^3$-AU.


Rather than Carter and Wegman's breath-first approach to tree hash, which uses $\Theta(n)$ space on each execution, or a recursive approach that proved to be prohibitively slow, HalftimeHash uses a depth-first post-order using iteration rather than recursion.
The iteration creates a stack of levels for each of the 3 instances.
Partial results are collected in these stack levels until there are $f$ items in a level, which are then hashed and the result deposited in the next level.
This $f$ is a user-tunable quantity which trades off input entropy for instruction-level parallelism, discussed further in Section \ref{analysis}.

The $f$ items are hashed using an NH variant that does not hash the last pair, but that is nonetheless $2^{-32}$-AU: \cite{badger}

\[
\left(\sum_{i=0}^{m-1} (d_{2i} + s_{2i})(d_{2i+1} + s_{2i+1})\right) + d_{2m} + 2^{32} d_{2m+1}
\]

This saves an addition and a multiplication, improving the speed in practice.

Rather than tree hashing, hash families like Poly1305, clhash, and UMASH use polynomial hashing (based on Horner's method) to hash variable-length strings down to fixed-size output.
That approach requires 64-bit multiplication and also reduction modulo a prime (in $\ints$ or in $\ints_2[x]$), limiting its usability in SIMD ISA extensions.

\subsection{Sweep}

Once the tree hash portion of HalftimeHash is complete, there still remains data to hash.

First, the tree hash leaves data in its stack - as many as $bf$ words of data per level per tree, where $b$ is the size of a block.
The Badger hash family addresses this by promoting items from lower levels to upper levels following Equation~\ref{algebraic-badger}.
This does not negatively affect the collision probability, as promotion is equivalent to hashing with the identity function, which has collision probability $\varepsilon = 0$.

This approach uses no additional entropy, but it does create dependencies between hashing that is closer to the leaves of the stack and hashing that is closer to the root, and testing revealed it to be slower in some cases.
Instead, HalftimeHash uses NH to hash all of the data in the stack; see Figure \ref{no-badger}.
The stack is itself treated as input to NH.
This requires as much input entropy as the number of words in the stack.


In addition to the data on the stack, there are some characters that have yet to be hashed at all: HalftimeHash's EHC design expects to be fed $7 \cdot 3 = 21$ words at each invocation, and no fewer, so there may be up to 20 words remaining at the end of the input string.
Every word in this remainder is fed into into all three NH executions that were created when hashing the stack.

HalftimeHash then returns the three NH sums, for a total of 24 bytes of output.


\section{Analysis}
\label{analysis}

This section presents metrics of an execution of HalftimeHash, including the collision probability.%, number of multiplications performed, and the amount of input entropy used.
This section will treat HalftimeHash as abstract, rather than focusing on a single version as Section \ref{algo} did.
The parameters are:

\begin {description}
\item[$w$] is the number of blocks in each item used in the Encode step of EHC.
\item[$d$] is the number of elements before applying the encoding.
\item[$e$] is the {\em encoded size} - the number of blocks in EHC after applying the encoding.
\item[$k$] is the number of blocks produced by the Combine step of EHC.
  This is also the minimum distance of the code, as described above.
\item[$b$] the number of 64-bit words in a block.
\item[$f$] is fanout, the width of the NH instance at tree hash nodes.
\item[$p$] is the maximum power of 2 that divides a determinant of any $k \times k$ matrix made from columns of the matrix $T$; doubling $p$ increases $\varepsilon$ by a factor of $2^k$.
\end{description}

In the example from Section ~\ref{algo} and the code corresponding to it, \[(w, d, e, k, b, f, p) = (3, 7, 9, 3, 8, 8, 2)\]

\subsection{EHC}




In each EHC execution, $d w$ blocks are read in.
They are converted to $e w$ blocks and then those $e w$ blocks are then hashed with NH to $e$ blocks.
This uses $e w$ words of input entropy and performs $e w$ multiplications.


As for collision probability, Nandi proved that if the EHC matrix product is over a finite field, EHC is $2^{-32k}$-AU.
$\ints_{2^{64}}$ is not a finite field, but the structure of this proof that EHC in HalftimeHash is AU is similar to the structure of Nandi's proof, so a variant of that is presented here as a warm-up.  \cite{ehc-nandi}

\begin{definition}
  A hash family $H$ is said to be {\em $\varepsilon$-almost $\Delta$-universal} (or just A$\Delta$U) when
  \[
  \forall x,y,\delta, \Pr_s[H(s,x) - H(s,y) = \delta] \leq \varepsilon \in o(1)
  \]
\end{definition}

And NH is $2^{-32}$-A$\Delta$U. \cite{umac}


\begin{lemma}
  When the matrix product is taken over a field, if the hash function used in step 2 is $\varepsilon$-A$\Delta$U, EHC is $\varepsilon^k$-A$\Delta$U.
\end{lemma}
\begin{proof}
  Let $\bar{H}$ be defined as $\bar{H}(s, x)_i \defeq H(s_i, x_i)$.
  Let $J$ be the encoding function that acts on $x$ and $y$.
  Given that $x$ and $y$ differ, let $F$ be $k$ locations where $J(x)_i \neq J(y)_i$.
  Let $T|_F$ be the matrix formed by the columns of $T$ where the column index is in $F$ and let $\bar{H}|_F$ similarly be $\bar{H}$ restricted to the indices in $F$.
  Conditioning over the $e -k$ indices not in $F$, we want to bound
  \begin{equation}
    \label{ehc-delta}
    \Pr_s[T|_F \bar{H}|_F(s, J(x)) - T|_F \bar{H}|_F(s, J(y)) = \delta]
  \end{equation}
  Since any $k$ columns of $T$ are independent, $T|_F$ is non-singular, and the equation is equivalent to $\bar{H}|_F(s, J(x)) - \bar{H}|_F(s, J(y)) = {T|_F}^{-1} \delta$, which implies
  \[
  \bigwedge_{i \in F} H(s_i, J(x)_i) - H(s_i, J(y)_i) = \beta_i
  \]
  where $\beta \defeq {T|_F}^{-1} \delta$.

  Since the $s_i$ are all chosen independently, the probability of the conjunction is the product of the probabilities, showing
  \[
  \begin{array}{rl}
    &  \Pr_s[T|_F \bar{H}|_F (s,J(x)) - T|_F \bar{H}|_F(s,J(y)) = \delta] \\
  \leq &  \prod_{i \in F} Pr_s[H(s_i, J(x)_i) - H(s_i, J(y)_i) = \beta_i]
  \end{array}
  \]
  and since $H$ is A$\Delta$U, this probability is $\varepsilon^k$.  \qed
\end{proof}

Note that this lemma depends on $k$ being the minimum distance of the code.
If the distance were less than $k$, then either the matrix would be smaller in both dimensions, increasing the probability of collisions, or the matrix would have fewer columns than rows, causing the matrix to be singular.

\begin{theorem}
  The EHC step of HalftimeHash is $2^{k(p-32)}$-A$\Delta$U.
\end{theorem}

\begin{proof}
  In HalftimeHash, the proof of the lemma above unravels at the reliance upon the trivial kernel of $T|_F$.
  The columns of $T$ in HalftimeHash are linearly independent, so the matrix $T|_F$ is injective in rings without zero dividers, but not necessarily injective in $\ints_{2^{64}}$.

  However, even in $\ints_{2^{64}}$, the adjugate matrix $\adj(A)$ has the property that $A \cdot \adj(A) = \adj(A) \cdot A = \det(A) I$.
  Let $\det(T|_F) = q2^{p'}$, where $q$ is odd and $p' \le p$.
  Now (\ref{ehc-delta}) reduces to
  \[
  \begin{array}{rl}
    &   \Pr_s[T|_F \bar{H}|_F(s,x) - T|_F \bar{H}|_F(s,y) = \delta]\\
    \leq &  \Pr_s[\adj(T|_F) T|_F \bar{H}|_F(s,x) - \adj(T|_F) T|_F \bar{H}|_F(s,y) = \adj(T|_F) \delta] \\
    = &  \Pr_s[q2^{p'}\bar{H}|_F(s,x) - q2^{p'}\bar{H}|_F(s,y) = \adj(T|_F) \delta] \\
    = &  \Pr_s[2^{p'}\bar{H}|_F(s,x) - 2^{p'}\bar{H}|_F(s,y) = q^{-1} \adj(T|_F) \delta]
  \end{array}
  \]

  Now letting $\beta = q^{-1} \adj(T|_F) \delta$ and letting the modulo operator extend pointwise to vectors, we have

  \[
  \begin{array}{rl}
    = &  \Pr_s[\bar{H}|_F(s,x) - \bar{H}|_F(s,y) \equiv \beta \bmod 2^{64-p'}] \\
    = &  \Pr_s\left[\bigwedge_{i \in F} H(s_i,x_i) - H(s_i,y_i) \equiv \beta_i \bmod 2^{64-p'}\right] \\
    = &  \prod_{i \in F} \Pr_s\left[ H(s_i,x_i) - H(s_i,y_i) \equiv \beta_i \bmod 2^{64-p'}\right] \\
    = & \left(2^{p'} 2^{-32}\right)^{|F|} = 2^{k(p'-32)}
  \end{array}
  \]
  This quantity is highest when $p'$ is at its maximum over all potential sets of columns $F$, and $p'$ is at most $p$, by the definition of $p$.

  \qed
  % TODO: this can be reduced by shuffling the column order
\end{proof}





\subsection{Tree hash}

For the tree hash portion of HalftimeHash, the height of the $k$ trees drives multiple metrics.
Each tree has $\lfloor n / b d w \rfloor$ blocks as input and, when a block is written to level $i$ for the first time, the execution is a complete $f$-ary tree.
The height of the tree is thus $\left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$.

\begin{lemma}
The tree hash portion of EHC is $2^{ k\lg\left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor - 32k}$-AU.
\end{lemma}
\begin{proof}
  Carter and Wegman showed that tree hash has collision probability of $m \varepsilon$, where $\varepsilon$ is the collision probability of a single node in the tree and $m$ is the height of the tree.
  Each tree node uses NH, so a single tree has collision probability $\left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor 2^{-32}$.
  A collision occurs for HalftimeHash at the tree hash stage if and only if each tree has a collision, which has probability $\left(\left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor 2^{-32}\right)^k$, assuming that the EHC step didn't already induce a collision. \qed
\end{proof}


The amount of entropy needed is proportional to the height of the tree, with $f - 1$ words needed for every level.
HalftimeHash uses different seeds for the $k$ different trees, so the total number of 64-bit words of entropy used in the tree hash step is $(f - 1) k\left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor$.

The number of multiplications performed is identical to the number of input words. $k b \lfloor n / b d w \rfloor$.


\begin{tabular}{|r|c|c|}
  \hline & {\bf EHC} & {\bf Tree hash}\\
  \hline {\bf Multiplications (each node)} & $b e w$ & $b (f-1)$ \\
  \hline {\bf Multiplications (total)} & $b e w \lfloor n / b d w\rfloor$ & $k b \lfloor n / b d w \rfloor$ \\
  \hline {\bf In Entropy (each tree $\times$ level)} & N/A & $f-1$ \\
  \hline {\bf In Entropy (total)} & $e w$ & $k (f-1) \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$ \\
  \hline {\bf Out Entropy (total)} & $k (32-p)$ & $32k - k\lg\left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor$\\
  \hline {\bf Output words (total)} & $k b \lfloor n / b d w\rfloor $ & $b f k \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$\\
  \hline
\end{tabular}


\subsection{Sweep}

For the output words in the stack after tree hash, HalftimeHash uses NH, which uses $b f k \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor$ words of entropy and just as many multiplications.

There can also be as much as $b d w$ words of data in the raw input that are untouched so far by HalftimeHash.
Again, NH is used, but now hashing $k$ times, since this data has not gone through EHC.
That requires $b d w k$ words of entropy and just as many multiplications.

For the untouched data, the number of words of entropy needed can be reduced by nearly a factor of $k$ using the Toeplitz construction.
Let $r$ be the sequence of random words used in the leftover part of the sweep.
Instead of using $[r_{i b d w}, r_{(i+1)b d w})$ as the keys to hash component $i$ with, HalftimeHash uses $[r_{i}, r_{b d w + i})$.
Both Nandi and Woelfel give proofs that this construction for multi-part hash output is A$\Delta$U. \cite{ehc-nandi,woelfel-toeplitz}

\subsection{Cumulative analysis}

The combined collision probability is
\[2^{k(p-32)} + \left\lfloor \log_f \lfloor n / b d w \rfloor \right\rfloor^k 2^{-32k} + 2^{-32k}\]
For the variant described in Section~\ref{algo}, and for strings less than an exabyte in length, this is more than 83 bits of entropy.

The combined input entropy needed (in words) is

\[
e w
+ (f-1) k \left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor
+ b f k \left\lfloor\log_f \lfloor n/b d w\rfloor\right\rfloor
+ b d w + k - 1
\]

The variant from Section~\ref{algo} requires 8.4KB for strings of length up to one megabyte.
6.1KB of this is for hashing the stack, which could be eliminated by using the pass-through mechanism of Badger, discussed above.
For strings of length one exabyte, the entropy required is 34KB.

The number of multiplications is dominated by the EHC step, since the total is $(e w + k) b \lfloor n / b d w \rfloor + O(\log n)$ and $e w$ is significantly larger than $k$.
For a string of length 1MB, 84\% of the multiplications happen in the EHC step. %, and the number of multiplications is about one per ten bytes of input.
Intel's VTune tool show the same thing: 86\% of the clock cycles are spent in the EHC step.

Similarly, clhash and UMASH, which are based on 64-bit carryless NH, have their execution times dominated by the multiplications in their base step.~\cite{clhash,umash}
These instructions are not yet available in as vector instructions, and not all chips have carryless multiplications.
HalftimeHash could be extended to carryless multiplication with no change in the analysis.




\section{Tests}

This section covers performance testing for HalftimeHash, especially compared to clhash and UMASH, the two fastest AU families on long strings.
Each of those are based on NH over $\ints_2[x]$, rather than $\ints_{2^{64}}$.

HalftimeHash passes all correctness and randomness tests in the SMHasher test suite; for a performance comparison, see Figure \ref{smhasher-speed} and \cite{smhasher}.

\begin{figure}
  \includegraphics[width=\textwidth]{smhasher-speed.eps}
\caption{
  \label{smhasher-speed}
    The two fastest variants of HalftimeHash are faster than all hash families in the SMHasher suite on 256KiB strings on an i7-7800x, even families that come with no AU guarantees. \protect\cite{smhasher}
    Of the families here, only HalftimeHash and xxh128 pass all SMHasher tests, and only HalftimeHash and clhash are AU.
}
\end{figure}

Figure \ref{frontier} displays the relationship between output entropy and throughput.
Adding more output entropy increases the number of multiplications and additions that HalftimeHash (or clhash, or UMASH) has to perform.
Nandi showed that this is true in the general case, as there is a matching upper and lower bound for the number of non-linear operations to be performed for a certain hash output width. \cite{ehc-nandi}


\begin{figure}
\includegraphics[width=\textwidth]{speed-v-epsilon.eps}
\caption{
  \label{frontier}
  Trade-offs for almost-universal string hashing functions on strings of size 250KB on an i7-7800x.
  UMASH comes in two variants based on the output width in bits, clhash doesn't but running clhash twice is included in the chart.
  For each clhash / UMASH version, at least one version of HalftimeHash is faster and has lower collision probability. \protect\cite{layer-of-maxima}
}
\end{figure}

Figure \ref{vs-cl} adds comparisons between clhash, UMASH, and HalftimeHash across input sizes and ISA's.
Although these machines support different ISA vector extensions, the pattern is similar: at high enough volume, HalftimeHash's throughput exceeds that of the polynomial multiplication families.
The ``v3'' after the name indicates block size: v3 means a 256-bit block size, while v4 (the default) means 512-bit block size.
V3 is used in the AMD tests, as AMD does not yet support AVX-512.


\begin{figure*}
\begin{tabular}{cccc}
\includegraphics[width=6.0cm]{line-cl-hh24.eps}
&
\includegraphics[width=6.0cm]{amd-cl-hh24.eps}
\end{tabular}
\caption{
  \label{vs-cl}
  Comparison of Intel (i7-7800x) and AMD (EC2 c5a.large, 7R32 chip) performance.
  These AMD chips do not support AVX-512, but still HalftimeHash with 256-bit blocks exceeds the speed of clmul-based hashing methods by up to a factor of 2 on long strings.
  In both cases, for long strings, HalftimeHash with 24 bytes of output is faster than clhash and UMASH.
  HalftimeHash24 also has lower collision probability.
}
\end{figure*}

\section{Future work}

Future potential work not already mentioned includes:

\begin{itemize}
\item HalftimeHash is a fast family for strings that are more than 1KB in size.
  Above was pondered combining HalftimeHash with hash families designed for shorter strings.
  The details of that combination remain future work.
\item HalftimeHash depends on platforms supporting widening multiplication of two 32-bit integers that produces a 64-bit integer.
  This operation is present in many environments, but not all.
  Notable exceptions include some embedded platforms that do not support {\em widening} multiplication of two 32-bit integers as well as native JavaScript, which does not support 64 bit integers at all.
\item More detailed comparisons against hash algorithms written for speed and executed in the Linux kernel, including Poly1305 and \texttt{crc32\_pclmul\_le\_16} could also be a fruitful direction for future work, as could comparisons on POWER and ARM ISA's.
\end{itemize}


\section*{Acknowledgments}
Thanks to Daniel Lemire, Paul Khuong, and Guy Even for helpful discussions and feedback.


\bibliographystyle{splncs04}
\bibliography{library}


\end{document}
\endinput

%%  LocalWords:  HalftimeHash codomain ISA's UMASH falkhash MeowHash
%%  LocalWords:  MetroHash FarmHash clhash wyhash farmhash UHASH EHC
%%  LocalWords:  VHASH Nandi's ISA strided Wegman TODO VTune NH's XXH
%%  LocalWords:  UMAC VMAC carryless fanout Nandi Gabrielyan Toeplitz
%%  LocalWords:  Gabrielyan's HalftimeHash's Woelfel SMHasher PVLDB
%%  LocalWords:  VLDB zeroless Skylake uint clmul pclmul Lemire xxh
%%  LocalWords:  Khuong Wegman's tunable
